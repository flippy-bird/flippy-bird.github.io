{"meta":{"title":"大潘子的文字角落","subtitle":"","description":"","author":"Pan","url":"https://flippy-bird.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-11-26T05:54:56.000Z","updated":"2025-11-26T05:55:08.826Z","comments":true,"path":"categories/index.html","permalink":"https://flippy-bird.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-11-26T05:53:10.000Z","updated":"2025-11-26T12:54:36.650Z","comments":true,"path":"tags/index.html","permalink":"https://flippy-bird.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"LLM的plan调研","slug":"LLM的plan调研","date":"2025-12-08T06:58:33.000Z","updated":"2025-12-08T12:03:07.446Z","comments":true,"path":"2025/12/08/LLM的plan调研/","permalink":"https://flippy-bird.github.io/2025/12/08/LLM%E7%9A%84plan%E8%B0%83%E7%A0%94/","excerpt":"","text":"1. langchain&#x2F;langgraph 参考资料： https://docs.langchain.com/oss/python/langchain/middleware/built-in#to-do-list langchain的思想是通过middleware的形式来添加plan的能力，可以参考源码部分：langchain&#x2F;libs&#x2F;langchain_v1&#x2F;langchain&#x2F;agents&#x2F;middleware&#x2F;todo.py （12月8日，后续不知道会不会变化）其本质仍然是一个工具函数的调用，内部的一个工具是write_todos, 相关的middleware是 TodoListMiddleware 这个工具的具体定义是：关于write_todos这个工具的描述，这个描述写的很长，具体可以去源码查看； 123456789@tool(description=WRITE_TODOS_TOOL_DESCRIPTION)def write_todos(todos: list[Todo], tool_call_id: Annotated[str, InjectedToolCallId]) -&gt; Command: &quot;&quot;&quot;Create and manage a structured task list for your current work session.&quot;&quot;&quot; return Command( update=&#123; &quot;todos&quot;: todos, &quot;messages&quot;: [ToolMessage(f&quot;Updated todo list to &#123;todos&#125;&quot;, tool_call_id=tool_call_id)], &#125; ) system prompt是： 12345678910111213WRITE_TODOS_SYSTEM_PROMPT = &quot;&quot;&quot;## `write_todos`You have access to the `write_todos` tool to help you manage and plan complex objectives.Use this tool for complex objectives to ensure that you are tracking each necessary step and giving the user visibility into your progress.This tool is very helpful for planning complex objectives, and for breaking down these larger complex objectives into smaller steps.It is critical that you mark todos as completed as soon as you are done with a step. Do not batch up multiple steps before marking them as completed.For simple objectives that only require a few steps, it is better to just complete the objective directly and NOT use this tool.Writing todos takes time and tokens, use it when it is helpful for managing complex many-step problems! But not for simple few-step requests.## Important To-Do List Usage Notes to Remember- The `write_todos` tool should never be called multiple times in parallel.- Don&#x27;t be afraid to revise the To-Do list as you go. New information may reveal new tasks that need to be done, or old tasks that are irrelevant.&quot;&quot;&quot; 2. Qwen code https://github.com/QwenLM/qwen-code 采用的是内置工具的方式来实现agent的plan能力，源代码部分可参考：qwen-code&#x2F;packages&#x2F;core&#x2F;src&#x2F;tools&#x2F;todoWrite.ts 具体的提供了下面的todo工具类： 12345678910class TodoWriteToolInvocation extends BaseToolInvocation&lt; TodoWriteParams, ToolResult&gt; &#123;&#125;// 暴露给外面的todo工具类export class TodoWriteTool extends BaseDeclarativeTool&lt; TodoWriteParams, ToolResult&gt; &#123;&#125; 关于工具的描述，截取一部分prompt如下： 1234567891011121314151617181920212223242526272829const todoWriteToolDescription = `Use this tool to create and manage a structured task list for your current coding session. This helps you track progress, organize complex tasks, and demonstrate thoroughness to the user.It also helps the user understand the progress of the task and overall progress of their requests.## When to Use This ToolUse this tool proactively in these scenarios:1. Complex multi-step tasks - When a task requires 3 or more distinct steps or actions2. Non-trivial and complex tasks - Tasks that require careful planning or multiple operations3. User explicitly requests todo list - When the user directly asks you to use the todo list4. User provides multiple tasks - When users provide a list of things to be done (numbered or comma-separated)5. After receiving new instructions - Immediately capture user requirements as todos6. When you start working on a task - Mark it as in_progress BEFORE beginning work. Ideally you should only have one todo as in_progress at a time7. After completing a task - Mark it as completed and add any new follow-up tasks discovered during implementation## When NOT to Use This ToolSkip using this tool when:1. There is only a single, straightforward task2. The task is trivial and tracking it provides no organizational benefit3. The task can be completed in less than 3 trivial steps4. The task is purely conversational or informationalNOTE that you should not use this tool if there is only one trivial task to do. In this case you are better off just doing the task directly.## Examples of When to Use the Todo List ...`; 3. gemini cli和qwen code 类似，源码部分在：gemini-cli&#x2F;packages&#x2F;core&#x2F;src&#x2F;tools&#x2F;write-todos.ts 1234export class WriteTodosTool extends BaseDeclarativeTool&lt; WriteTodosToolParams, ToolResult&gt; &#123;&#125; 工具说明的prompt截取如下： 123456789101112131415161718192021222324252627282930export const WRITE_TODOS_DESCRIPTION = `This tool can help you list out the current subtasks that are required to be completed for a given user request. The list of subtasks helps you keep track of the current task, organize complex queries and help ensure that you don&#x27;t miss any steps. With this list, the user can also see the current progress you are making in executing a given task.Depending on the task complexity, you should first divide a given task into subtasks and then use this tool to list out the subtasks that are required to be completed for a given user request.Each of the subtasks should be clear and distinct. Use this tool for complex queries that require multiple steps. If you find that the request is actually complex after you have started executing the user task, create a todo list and use it. If execution of the user task requires multiple steps, planning and generally is higher complexity than a simple Q&amp;A, use this tool.DO NOT use this tool for simple tasks that can be completed in less than 2 steps. If the user query is simple and straightforward, do not use the tool. If you can respond with an answer in a single turn then this tool is not required.## Task state definitions- pending: Work has not begun on a given subtask.- in_progress: Marked just prior to beginning work on a given subtask. You should only have one subtask as in_progress at a time.- completed: Subtask was successfully completed with no errors or issues. If the subtask required more steps to complete, update the todo list with the subtasks. All steps should be identified as completed only when they are completed.- cancelled: As you update the todo list, some tasks are not required anymore due to the dynamic nature of the task. In this case, mark the subtasks as cancelled.## Methodology for using this tool1. Use this todo list as soon as you receive a user request based on the complexity of the task.2. Keep track of every subtask that you update the list with.3. Mark a subtask as in_progress before you begin working on it. You should only have one subtask as in_progress at a time.4. Update the subtask list as you proceed in executing the task. The subtask list is not static and should reflect your progress and current plans, which may evolve as you acquire new information.5. Mark a subtask as completed when you have completed it.6. Mark a subtask as cancelled if the subtask is no longer needed.7. You must update the todo list as soon as you start, stop or cancel a subtask. Don&#x27;t batch or wait to update the todo list.## Examples of When to Use the Todo List ...`; 4. AgentScope https://github.com/agentscope-ai/agentscope 也是通过工具调用的方式来实现的，可以参考我之前的文档 AgentScope源码学习 5.cline https://github.com/cline/cline 应该也是通过调用的工具的方式让Agent具备了plan的能力，相关的提示词源码是：cline&#x2F;src&#x2F;core&#x2F;prompts&#x2F;system-prompt&#x2F;components&#x2F;task_progress.ts 对应的prompt如下：可以看到， 12345678910111213141516171819202122232425const UPDATING_TASK_PROGRESS = `UPDATING TASK PROGRESSYou can track and communicate your progress on the overall task using the task_progress parameter supported by every tool call. Using task_progress ensures you remain on task, and stay focused on completing the user&#x27;s objective. This parameter can be used in any mode, and with any tool call.- When switching from PLAN MODE to ACT MODE, you must create a comprehensive todo list for the task using the task_progress parameter- Todo list updates should be done silently using the task_progress parameter - do not announce these updates to the user- Use standard Markdown checklist format: &quot;- [ ]&quot; for incomplete items and &quot;- [x]&quot; for completed items- Keep items focused on meaningful progress milestones rather than minor technical details. The checklist should not be so granular that minor implementation details clutter the progress tracking.- For simple tasks, short checklists with even a single item are acceptable. For complex tasks, avoid making the checklist too long or verbose.- If you are creating this checklist for the first time, and the tool use completes the first step in the checklist, make sure to mark it as completed in your task_progress parameter.- Provide the whole checklist of steps you intend to complete in the task, and keep the checkboxes updated as you make progress. It&#x27;s okay to rewrite this checklist as needed if it becomes invalid due to scope changes or new information.- If a checklist is being used, be sure to update it any time a step has been completed.- The system will automatically include todo list context in your prompts when appropriate - these reminders are important.Example:&lt;execute_command&gt;&lt;command&gt;npm install react&lt;/command&gt;&lt;requires_approval&gt;false&lt;/requires_approval&gt;&lt;task_progress&gt;- [x] Set up project structure- [x] Install dependencies- [ ] Create components- [ ] Test application&lt;/task_progress&gt;&lt;/execute_command&gt;` 对于Vscode中ide部分，貌似是通过一个focus chain模块来实现的，具体部分可参考源代码文件： cline&#x2F;src&#x2F;core&#x2F;task&#x2F;focus-chain&#x2F;","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"plan","slug":"plan","permalink":"https://flippy-bird.github.io/tags/plan/"}]},{"title":"Mem0源码学习","slug":"Mem0源码学习","date":"2025-12-01T05:54:24.000Z","updated":"2025-12-05T10:19:41.851Z","comments":true,"path":"2025/12/01/Mem0源码学习/","permalink":"https://flippy-bird.github.io/2025/12/01/Mem0%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"https://github.com/mem0ai/mem0 1. 项目的核心代码从memory&#x2F;base.py 里面的基类可以看到，记忆功能主要实现了下面的功能 123456789101112131415class MemoryBase(ABC) @abstractmethod def get(self, memory_id): # 根据id获取对应id的memory @abstractmethod def get_all(self): # 列出所有的memory @abstractmethod def update(self, memory_id, data): # 更新id的memory @abstractmethod def delete(self, memory_id): # 删除id的memory @abstractmethod def history(self, memory_id): # Get the history of changes for a memory by ID. mem0的核心实现在memory&#x2F;main.py 文件中 123456class Memory(MemoryBase): # 这里除了实现上面base定义的基本功能外，有一个search的核心功能实现 def search(self, query, ...): # 这里会根据用户的query去寻找最相关的memory class AsyncMemory(MemoryBase): # 异步的接口不用再使用另外一个基类，直接使用了MemoryBase，但是在实现时使用了异步 async def get(self, memory_id): 1.1 search看search里面的实现，就是标准的RAG流程，embedding query, 然后从向量数据库或者图数据库召回，做一个ReRank重排序 123456789101112131415# 是通过下面的函数来实现向量召回的def _search_vector_store(self, query, filters, limit, threshold: Optional[float] = None): embeddings = self.embedding_model.embed(query, &quot;search&quot;) # 这一步就是向量检索了 memories = self.vector_store.search(query=query, vectors=embeddings, limit=limit, filters=filters) ... def search(self, query, ...): ... with concurrent.futures.ThreadPoolExecutor() as executor: future_memories = executor.submit(self._search_vector_store, query, effective_filters, limit, threshold) ## 这里的self.graph就是图关系库 future_graph_entities = (executor.submit(self.graph.search, query, effective_filters, limit) if self.enable_graph else None) 关于Memory类中使用到的LLM，embedding, graph,rerank模型等等都是使用工厂模式来初始化的 12345678910111213141516171819202122class Memory(MemoryBase): self.embedding_model = EmbedderFactory.create( self.config.embedder.provider, self.config.embedder.config, self.config.vector_store.config, ) self.vector_store = VectorStoreFactory.create( self.config.vector_store.provider, self.config.vector_store.config ) self.llm = LlmFactory.create(self.config.llm.provider, self.config.llm.config) if config.reranker: self.reranker = RerankerFactory.create( config.reranker.provider, config.reranker.config ) if self.config.graph_store.config: provider = self.config.graph_store.provider self.graph = GraphStoreFactory.create(provider, self.config) self.enable_graph = True else: self.graph = None 工厂函数里面对应的就是各个可以支持的实现，这个实现是在 mem0&#x2F;utils&#x2F;factory.py 文件中，下面的是图关系数据库的支持代码 12345678910111213141516171819202122class GraphStoreFactory: &quot;&quot;&quot; Factory for creating MemoryGraph instances for different graph store providers. Usage: GraphStoreFactory.create(provider_name, config) &quot;&quot;&quot; provider_to_class = &#123; &quot;memgraph&quot;: &quot;mem0.memory.memgraph_memory.MemoryGraph&quot;, &quot;neptune&quot;: &quot;mem0.graphs.neptune.neptunegraph.MemoryGraph&quot;, &quot;neptunedb&quot;: &quot;mem0.graphs.neptune.neptunedb.MemoryGraph&quot;, &quot;kuzu&quot;: &quot;mem0.memory.kuzu_memory.MemoryGraph&quot;, &quot;default&quot;: &quot;mem0.memory.graph_memory.MemoryGraph&quot;, &#125; @classmethod def create(cls, provider_name, config): class_type = cls.provider_to_class.get(provider_name, cls.provider_to_class[&quot;default&quot;]) try: GraphClass = load_class(class_type) except (ImportError, AttributeError) as e: raise ImportError(f&quot;Could not import MemoryGraph for provider &#x27;&#123;provider_name&#125;&#x27;: &#123;e&#125;&quot;) return GraphClass(config) 可以看到mem0支持4中图关系数据库memgraph、neptune、neptunedb、kuzu(为什么会放在两个不同文件夹下面呢？这个暂时还没看明白)，其它的LLM，embedding等等类似； 1.2 add这个函数是mem0的核心,主要通过两个子函数来实现 1234567891011121314151617181920def add(self, messages, *, user_id: Optional[str] = None, agent_id: Optional[str] = None, run_id: Optional[str] = None, metadata: Optional[Dict[str, Any]] = None, infer: bool = True, memory_type: Optional[str] = None, prompt: Optional[str] = None,): with concurrent.futures.ThreadPoolExecutor() as executor: # 处理embedding相关的 future1 = executor.submit(self._add_to_vector_store, messages, processed_metadata, effective_filters, infer) # 图关系数据库 future2 = executor.submit(self._add_to_graph, messages, effective_filters) concurrent.futures.wait([future1, future2]) vector_store_result = future1.result() graph_result = future2.result() 而核心中的核心是_add_to_vector_store 这个函数 1234567891011121314151617181920212223242526# 当 infer 为false时，是常规的记忆存储，embedding,入库 (直接入库的模式)def _add_to_vector_store(self, messages, metadata, filters, infer): # 当infer 为true时，将会使用LLM分析对话内容，提取有意义的事实，并智能地决定如何处理这些记忆（添加、更新、删除或保持不变） ### 这里是提取实体关系的逻辑 parsed_messages = parse_messages(messages) if self.config.custom_fact_extraction_prompt: system_prompt = self.config.custom_fact_extraction_prompt user_prompt = f&quot;Input:\\n&#123;parsed_messages&#125;&quot; else: # Determine if this should use agent memory extraction based on agent_id presence # and role types in messages is_agent_memory = self._should_use_agent_memory_extraction(messages, metadata) system_prompt, user_prompt = get_fact_retrieval_messages(parsed_messages, is_agent_memory) response = self.llm.generate_response( messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_prompt&#125;, ], response_format=&#123;&quot;type&quot;: &quot;json_object&quot;&#125;, ) # 抽取实体后，开始进行判断下一步对记忆进行 增删改查的逻辑了，也是借助了大模型来实现 # 这部分太长省略了 可以观察一下mem0中抽取用户或者Agent 特性的提示词 (太长，只展示一部分，具体文件在：mem0&#x2F;configs&#x2F;prompts.py 里面的USER_MEMORY_EXTRACTION_PROMPT和AGENT_MEMORY_EXTRACTION_PROMPT) 首先，LLM主要任务是： 123456789101112131415161718AGENT_MEMORY_EXTRACTION_PROMPT = f&#x27;&#x27;&#x27;You are a Personal Information Organizer, specialized in accurately storing facts, user memories, and preferences. Your primary role is to extract relevant pieces of information from conversations and organize them into distinct, manageable facts. This allows for easy retrieval and personalization in future interactions. Below are the types of information you need to focus on and the detailed instructions on how to handle the input data.# [IMPORTANT]: GENERATE FACTS SOLELY BASED ON THE USER&#x27;S MESSAGES. DO NOT INCLUDE INFORMATION FROM ASSISTANT OR SYSTEM MESSAGES.# [IMPORTANT]: YOU WILL BE PENALIZED IF YOU INCLUDE INFORMATION FROM ASSISTANT OR SYSTEM MESSAGES.Types of Information to Remember:1. Store Personal Preferences: Keep track of likes, dislikes, and specific preferences in various categories such as food, products, activities, and entertainment.2. Maintain Important Personal Details: Remember significant personal information like names, relationships, and important dates.3. Track Plans and Intentions: Note upcoming events, trips, goals, and any plans the user has shared.4. Remember Activity and Service Preferences: Recall preferences for dining, travel, hobbies, and other services.5. Monitor Health and Wellness Preferences: Keep a record of dietary restrictions, fitness routines, and other wellness-related information.6. Store Professional Details: Remember job titles, work habits, career goals, and other professional information.7. Miscellaneous Information Management: Keep track of favorite books, movies, brands, and other miscellaneous details that the user shares.&#x27;&#x27;&#x27; 具体的few shot例子如下：给了一些正例和反例(不用提取的例子) 12345678910111213141516171819202122AGENT_MEMORY_EXTRACTION_PROMPT = f&#x27;&#x27;&#x27;User: Hi.Assistant: Hello! I enjoy assisting you. How can I help today?Output: &#123;&#123;&quot;facts&quot; : []&#125;&#125;User: There are branches in trees.Assistant: That&#x27;s an interesting observation. I love discussing nature.Output: &#123;&#123;&quot;facts&quot; : []&#125;&#125;User: Hi, I am looking for a restaurant in San Francisco.Assistant: Sure, I can help with that. Any particular cuisine you&#x27;re interested in?Output: &#123;&#123;&quot;facts&quot; : [&quot;Looking for a restaurant in San Francisco&quot;]&#125;&#125;User: Yesterday, I had a meeting with John at 3pm. We discussed the new project.Assistant: Sounds like a productive meeting. I&#x27;m always eager to hear about new projects.Output: &#123;&#123;&quot;facts&quot; : [&quot;Had a meeting with John at 3pm and discussed the new project&quot;]&#125;&#125;User: Hi, my name is John. I am a software engineer.Assistant: Nice to meet you, John! My name is Alex and I admire software engineering. How can I help?Output: &#123;&#123;&quot;facts&quot; : [&quot;Name is John&quot;, &quot;Is a Software engineer&quot;]&#125;&#125;&#x27;&#x27;&#x27; 不知道这种写法会不会限制LLM输出用户不想要的，prompt里面存在类似下面的句子： 12# [IMPORTANT]: GENERATE FACTS SOLELY BASED ON THE USER&#x27;S MESSAGES. DO NOT INCLUDE INFORMATION FROM ASSISTANT OR SYSTEM MESSAGES.# [IMPORTANT]: YOU WILL BE PENALIZED IF YOU INCLUDE INFORMATION FROM ASSISTANT OR SYSTEM MESSAGES. 获取到哪些用户的习惯或者其它一些与用户相关信息之后，然后使用embedding匹配从memory中召回一些与这些信息相关的信息；具体的代码如下： 12345678910111213# new_retrieved_facts 即是从这条消息中获取到的用户偏好for new_mem in new_retrieved_facts: messages_embeddings = self.embedding_model.embed(new_mem, &quot;add&quot;) new_message_embeddings[new_mem] = messages_embeddings existing_memories = self.vector_store.search( query=new_mem, vectors=messages_embeddings, limit=5, filters=search_filters, ) for mem in existing_memories: retrieved_old_memory.append(&#123;&quot;id&quot;: mem.id, &quot;text&quot;: mem.payload.get(&quot;data&quot;, &quot;&quot;)&#125;) 回溯到相关的历史memory之后，然后使用LLM确定是否添加新的记忆，还是删除更新旧的记忆，具体的职责相关prompt如下 12345678910111213DEFAULT_UPDATE_MEMORY_PROMPT = &quot;&quot;&quot;You are a smart memory manager which controls the memory of a system.You can perform four operations: (1) add into the memory, (2) update the memory, (3) delete from the memory, and (4) no change.Based on the above four operations, the memory will change.Compare newly retrieved facts with the existing memory. For each new fact, decide whether to:- ADD: Add it to the memory as a new element- UPDATE: Update an existing memory element- DELETE: Delete an existing memory element- NONE: Make no change (if the fact is already present or irrelevant)&quot;&quot;&quot; few shot 的例子如下：展示的是删除这个操作的例子，完整的可以看mem0&#x2F;configs&#x2F;prompts.py 文件中的DEFAULT_UPDATE_MEMORY_PROMPT 12345678910111213141516171819202122232425262728293031323334353637DEFAULT_UPDATE_MEMORY_PROMPT = &quot;&quot;&quot;There are specific guidelines to select which operation to perform:3. **Delete**: If the retrieved facts contain information that contradicts the information present in the memory, then you have to delete it. Or if the direction is to delete the memory, then you have to delete it.Please note to return the IDs in the output from the input IDs only and do not generate any new ID.- **Example**: - Old Memory: [ &#123; &quot;id&quot; : &quot;0&quot;, &quot;text&quot; : &quot;Name is John&quot; &#125;, &#123; &quot;id&quot; : &quot;1&quot;, &quot;text&quot; : &quot;Loves cheese pizza&quot; &#125; ] - Retrieved facts: [&quot;Dislikes cheese pizza&quot;] - New Memory: &#123; &quot;memory&quot; : [ &#123; &quot;id&quot; : &quot;0&quot;, &quot;text&quot; : &quot;Name is John&quot;, &quot;event&quot; : &quot;NONE&quot; &#125;, &#123; &quot;id&quot; : &quot;1&quot;, &quot;text&quot; : &quot;Loves cheese pizza&quot;, &quot;event&quot; : &quot;DELETE&quot; &#125; ] &#125;&quot;&quot;&quot; 1.3 其它memory&#x2F;telemetry.py 文件，这个文件的作用类似埋点，用来记录用户使用mem0的数据情况（在gemini cli， qwen code中看到同样的文件）","categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"memory","slug":"memory","permalink":"https://flippy-bird.github.io/tags/memory/"}]},{"title":"LLM memory","slug":"LLM-memory","date":"2025-12-01T05:47:10.000Z","updated":"2025-12-04T10:46:28.378Z","comments":true,"path":"2025/12/01/LLM-memory/","permalink":"https://flippy-bird.github.io/2025/12/01/LLM-memory/","excerpt":"","text":"参考资料 LLM memory evaluation EverMemos MemOS zep memu","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"memory","slug":"memory","permalink":"https://flippy-bird.github.io/tags/memory/"}]},{"title":"LightRAG","slug":"LightRAG","date":"2025-11-28T07:06:12.000Z","updated":"2025-11-28T08:41:03.926Z","comments":true,"path":"2025/11/28/LightRAG/","permalink":"https://flippy-bird.github.io/2025/11/28/LightRAG/","excerpt":"","text":"1. 背景待详细看完完整论文后补充，大概就是GraphRag虽然效果比较好，但是速度慢，建立图的过程中消耗的token也很多，因此有了LightRag； 2. LightRAG的原理 深度解析比微软的GraphRAG简洁很多的LightRAG，一看就懂 先看了知乎上的一些解读，感觉就是简化版本的GraphRAG，在索引阶段的处理方式是差不多的，只是LightRAG将GraphRAG中比较慢的部分去掉了(社区报告部分，因此也去掉了global search)，数据处理流程是：先分块，然后提取实体和关系，然后入库，index部分的工作就完成了； 在查询阶段，提供了4中方式： 最基本的向量相似度匹配 local search: 根据用户的query生成一些low-level 的关键词，然后根据生成的关键词去建立好的图谱查询； global search: 根据用户的query生成一些high-level 的关键词，然后根据生成的关键词去建立好的图谱查询； 混合搜索：local search + global search 3. 源码解析 TODO","categories":[{"name":"RAG框架学习","slug":"RAG框架学习","permalink":"https://flippy-bird.github.io/categories/RAG%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"}]},{"title":"AgentScope源码学习","slug":"AgentScope源码学习","date":"2025-11-25T11:47:53.000Z","updated":"2025-11-26T10:30:30.554Z","comments":true,"path":"2025/11/25/AgentScope源码学习/","permalink":"https://flippy-bird.github.io/2025/11/25/AgentScope%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"AgentScope (阿里的) https://github.com/agentscope-ai/agentscope 记忆长期记忆部分使用了mem0这个工具，当然，代码里面也提到了，可以使用阿里自家的ReMe这个记忆框架 Agent这一块儿使用的是基本的React模式，输出最后的回答，也成了一个工具；多了一个 1234567891011121314### AgentBase# 在AgentBase里面有一个虚函数 replydef reply()### 在ReactAgentBase里面有基本的React框架def _reasoning() # 虚函数def _acting() # 虚函数### 在ReactAgent里面def reply(): 这里就类似OpenManus里面的step()函数的作用了，从源码来看，逻辑完全一样 self._reasoning() ... self._acting() 在Agent里面有一个observe，感觉是为了观察到外界信息准备的接口(用于多Agent之间的信息互动) 1234567891011async def observe(self, msg: Msg | list[Msg] | None) -&gt; None: &quot;&quot;&quot;Receive the given message(s) without generating a reply. Args: msg (`Msg | list[Msg] | None`): The message(s) to be observed. &quot;&quot;&quot; raise NotImplementedError( f&quot;The observe function is not implemented in&quot; f&quot; &#123;self.__class__.__name__&#125; class.&quot;, ) 多Agent互动在这个框架里面使用的是swarm模式，似乎比较简单，每个agent observe其它agent的输出，添加到自己的记忆里面去就好了 在src&#x2F;pipeline&#x2F;_msghub.py文件夹里面 （或者见类名时的说明） 123456789async def broadcast(self, msg: list[Msg] | Msg) -&gt; None: &quot;&quot;&quot;Broadcast the message to all participants. Args: msg (`list[Msg] | Msg`): Message(s) to be broadcast among all participants. &quot;&quot;&quot; for agent in self.participants: await agent.observe(msg) Interrupt(中断介入)这个好像还不错，可以看一下 Plan的实现实现了一些plan的功能函数，然后让Agent去调用和更改当前的plan","categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"}]},{"title":"smolagents源码学习","slug":"smolagents源码学习","date":"2025-11-25T08:43:25.000Z","updated":"2025-11-28T08:41:59.151Z","comments":true,"path":"2025/11/25/smolagents源码学习/","permalink":"https://flippy-bird.github.io/2025/11/25/smolagents%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"https://github.com/huggingface/smolagents memory 管理1234class AgentMemory: def __init__(self, system_prompt:str): self.system_prompt: SystemPromptStep = SystemPromptStep(system_prompt=system_prompt) self.steps: list[TaskStep | ActionStep | PlanningStep] = [] 将LLM执行过程的信息划分成了四个部分(主要，其它)： TaskStep: 与用户输入相关 (用户提问，上传图片等) SystemPromptStep: 系统的prompt PlanningStep: 与规划相关的记忆 (暂时还没遇到) ActionStep：当前送到LLM进行执行的信息 历史信息的获取 所以主要在于上面每种类型记忆数据 to_messages的实现 需要注意的是前后信息的完整性 1234567891011121314151617181920212223# agents.py line:1256def _step_stream( self, memory_step: ActionStep ) -&gt; Generator[ChatMessageStreamDelta | ToolCall | ToolOutput | ActionOutput]: &quot;&quot;&quot; Perform one step in the ReAct framework: the agent thinks, acts, and observes the result. Yields ChatMessageStreamDelta during the run if streaming is enabled. At the end, yields either None if the step is not final, or the final answer. &quot;&quot;&quot; memory_messages = self.write_memory_to_messages() # agents.py line:758def write_memory_to_messages( self, summary_mode: bool = False, ) -&gt; list[ChatMessage]: &quot;&quot;&quot; Reads past llm_outputs, actions, and observations or errors from the memory into a series of messages that can be used as input to the LLM. Adds a number of keywords (such as PLAN, error, etc) to help the LLM. &quot;&quot;&quot; messages = self.memory.system_prompt.to_messages(summary_mode=summary_mode) for memory_step in self.memory.steps: messages.extend(memory_step.to_messages(summary_mode=summary_mode)) return messages 边界处理当超过最大尝试次数（默认是20次）时，最后会总结19步的step，然后给出一个最终答案 12345678910111213141516# agent.py line:810def provide_final_answer(self, task: str) -&gt; ChatMessage: messages : 这里有一个专门针对这种情况的系统prompt messages += self.write_memory_to_messages()[1:] messages : 需要组装的post_messages try: chat_message: ChatMessage = self.model.generate(messages) return chat_message except Exception as e: return ChatMessage( role=MessageRole.ASSISTANT, content=[&#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;Error in generating final LLM output: &#123;e&#125;&quot;&#125;], ) Agent: CodeAgent 项目的CodeAgent模式算是这个项目里面比较新颖的一种方式了，调用工具的API是通过执行python代码的方式来执行的，但是感觉解析python的AST那部分，就感觉好复杂-_-#，没有FunctionCall的这种方式简洁了。 来源于Executable Code Actions Elicit Better LLM Agents，这篇论文主要的出发动机是当前LLM Agent通常通过以预定义的格式生成 JSON 或文本来生成Action，这通常受到约束动作空间（例如，预定义工具的范围）和受限灵活性（例如，无法组合多个工具）的限制。 链接：https://zhuanlan.zhihu.com/p/16341067315","categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"}]},{"title":"Hello World","slug":"hello-world","date":"2025-11-25T07:59:06.847Z","updated":"2025-11-26T10:30:27.200Z","comments":true,"path":"2025/11/25/hello-world/","permalink":"https://flippy-bird.github.io/2025/11/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"MCP","slug":"mcp介绍","date":"2025-11-25T06:02:24.000Z","updated":"2025-11-28T08:41:22.264Z","comments":true,"path":"2025/11/25/mcp介绍/","permalink":"https://flippy-bird.github.io/2025/11/25/mcp%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"MCP话不多说，直接上图即可 有MCP和没有MCP的区别，提升了效率 MCP的架构图：主要是由Host、Client和Server三部分组成 个人demo mcp demo 参考资料 MCP (Model Context Protocol)，一篇就够了。 python SDK的官方文档 测试MCP工具接入使用的地址：阿里MCP","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"MCP","slug":"MCP","permalink":"https://flippy-bird.github.io/tags/MCP/"}]},{"title":"大模型入门(组内分享)","slug":"大模型入门-组内分享","date":"2025-06-24T13:01:18.000Z","updated":"2025-11-28T02:05:03.478Z","comments":true,"path":"2025/06/24/大模型入门-组内分享/","permalink":"https://flippy-bird.github.io/2025/06/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8-%E7%BB%84%E5%86%85%E5%88%86%E4%BA%AB/","excerpt":"","text":"背景 2025年开年以来，大家或多或少都听过下面这些词：deepseek， AI， 人工智能，Agent，具身智能； AI到底发展到哪一步了，利用AI可以做哪些事情，或者作为技术人员，想要使用AI做一些事情，应该学习那些东西，这篇将和大家一起探讨下。 备注声明：该分享定位轻科普向，不会涉及大模型相关的底层原理，有些结论属于个人的理解和感悟，如需要深入交流，欢迎大家和我深入探讨。 大模型相关应用Prompt Engineer(提示词)Prompt 咒语 —-&gt; 激发LLM的潜能 2023年的某一天，当你开始和ChatGpt互动的那一刻开始，你就是一个Prompt Engineer了！ 当你不断试探GPT，使用各种策略，反问，试探，举个例子等等(专业一点就是few shot, zero-shot, cot等等)，最终得到正确答案时，你其实已经获得了一些prompt Engineer的训练，在下一次的问答中，你就能更快更好地得到你想要的答案； 当然啦，现在大模型也挺多的，确实有一些朴素的咒语框架可以让我们快速得到更好了的回答，你如果想了解一下，可以看看下面这个课程 (超爽中英!) 2025吴恩达最好的【提示词工程师】教程！附课件代码 DeepLearning.AI_大模型_LLM_Prompt_哔哩哔哩_bilibili 当然，上面说的咒语是GPT对话形式的，文生图，文生视频等等AIGC的应用，咒语可能需要另外习得(你芝麻开门可以开这个门，另外一个门可就不行了哦) SFT(微调) 大模型训练的时候使用的是公开的数据集，prompt咒语念得再好，也没办法，巧妇难为无米之炊，对于特定领域的问题大模型会出现幻觉(瞎回答) 大模型的效果出现一些问题，需要纠正大模型的错误 改变大模型回答的风格等等需要定制，就需要大模型微调 如果大家想要进阶动手微调一下的话，可以使用下面的框架，按照格式准备好数据即可，然后一键启动，上机器开始炼丹即可！ 目前主流的微调框架： https://github.com/hiyouga/LLaMA-Factory https://github.com/unslothai/unsloth https://github.com/modelscope/ms-swift RAGRAG &#x3D; LLM + 外置数据库 ​ 备注：这部分写于2025年4月16日 用过GPT的同学可能会了解到，GPT的知识具有时效性，比如GPT-4发布时间在2023年，那么GPT-4绝对无法帮你回答2024年之后的事情；再者，你有一个本地知识库的时候，你需要大模型结合这个知识库来回答你的问题时，你可能就需要RAG了； 右边严格来说不是RAG的流程，但是广义来讲，也算RAG，llm在其中扮演的是嘴替(总结)的作用；左边这个图是标准的RAG的流程，主要包括两个部分：index(入库) 和 query(出库) 如果大家想要深入学习这一部分的话，下面是一些可以进阶的部分： 当前两个主流的RAG搭建框架(RAG企业级开发)： langchain：https://github.com/langchain-ai/langchain llamaindex：https://github.com/run-llama/llama_index 可以学习的开源项目： QAnything: https://github.com/netease-youdao/QAnything RAGflow：https://github.com/infiniflow/ragflow/blob/main/README_zh.md 关于RAG的优化：RAG技术 AgentAgent &#x3D; LLM + 外部工具 上面的RAG是LLM和外部的知识(文档，图片等)打通了一条链路，相当于在数据层面建立了联系；但是不具备数据处理的能力，而处理数据的能力，一般是API是通过API的形式来展现的；**因此当LLM能够使用外部的 工具(API), LLM的能力将得到极大的扩展 ，**不多bb，展示！！！ https://strudel.cc/ (下面Agent使用到的一个音乐工具) 如果没有耐心看完前面Agent执行步骤的话，可直接拉到2：48秒，然后看后面的即可 如果给了LLM一台可以运行的电脑(环境)，那么这就是今年3月份爆火的AI智能体 Manus 为什么要叫Agent呢，可以看下面的图(没找到比较好的图，自己画的，见谅)，我们可以将用户比作boss，llm比作员工，boss发出一个问题之后，llm去规划并完成工作，llm是具体的执行者，因此叫做代理,Agent 暂时无法在飞书文档外展示此内容 目前智能Agent产品设计一般都是按照上面的思路去进行的，规划(Plan) + 执行(ReAct模型) Agent通过上面的章节，对Agent进行了一些初步的了解，这个小节将通过代码进一步的带大家了解Agent相关的内容 Agent 李宏毅讲Agent 从上面的一些例子我们可以了解到，对于完成一个人类目标而言，Agent需要两个方面的能力，目标拆解的能力和分布执行的能力；对应的名词即plan和function call，有了分步的任务，之后，LLM Agent逐步分析，调用工具或者自身内部知识，然后将结果传给下一步的任务，如下图所示： 通过上面的方式，可以有哪些运用呢？ 用模型训练模型： 使用Agent下围棋，使用电脑，浏览网页等等！ 当然上面的例子也说明了，大模型在调用过程中，可能出现幻觉，这有可能导致任务执行的失败，比如上面的deepseek 改变了国际象棋的规则，然后赢得了AI届的国际象棋桂冠！因此对于复杂问题，选择其它方法或许更可靠。这里不再展开。 上面都提到了大模型要实现Agent，需要具备工具调用的能力，这个能力是怎么实现的呢？看图好像挺简单的一句话就说完了，其实实际呢，也挺简单的。下面通过一个Function Call的例子来简单说明一下。 Function call 调用的实例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import randomfrom datetime import datetimeimport json# step 1: 定义工具def get_current_time(): current_datetime = datetime.now() formatted_time = current_datetime.strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;) return f&quot;当前时间：&#123;formatted_time&#125;。&quot;def add(arguments): a = arguments[&quot;num_1&quot;] b = arguments[&quot;num_2&quot;] return f&quot;计算的结果是：&#123;a + b&#125;&quot;# step 2: 创建tools数组tools = [ &#123; &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: &#123; &quot;name&quot;: &quot;get_current_time&quot;, &quot;description&quot;: &quot;当你想知道现在的时间时非常有用。&quot;, &#125; &#125;,]tool_name = [tool[&quot;function&quot;][&quot;name&quot;] for tool in tools]# step 3：使用大模型调用函数from openai import OpenAIimport osclient = OpenAI( api_key=&quot;sk-389c222d8f304e6ba3bb10ad3589d340&quot;, base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,)messages = [ &#123; &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&quot;&quot;你是一个很有帮助的助手。如果用户提问关于时间的问题，请调用‘get_current_time’函数。 请以友好的语气回答问题。&quot;&quot;&quot;, &#125;, &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;现在是几点？&quot; &#125;]def function_calling(): completion = client.chat.completions.create( model=&quot;qwen-max&quot;, messages=messages, tools=tools, ) res = completion.choices[0].message print(res.model_dump_json()) return completion# step4: 运行工具函数completion = function_calling()function_name = completion.choices[0].message.tool_calls[0].function.namearguments_string = completion.choices[0].message.tool_calls[0].function.arguments# 使用json模块解析参数字符串arguments = json.loads(arguments_string)# 创建一个函数映射表function_mapper = &#123; &quot;get_current_time&quot;: get_current_time, &quot;add&quot;:add&#125;# 获取函数实体function = function_mapper[function_name]# 如果入参为空，则直接调用函数if arguments == &#123;&#125;: function_output = function()# 否则，传入参数后调用函数else: function_output = function(arguments)# 打印工具的输出print(f&quot;工具函数输出：&#123;function_output&#125;\\n&quot;)## step5： 将工具输出添加到messages中，继续进行下面的步骤messages.append(completion.choices[0].message)messages.append(&#123;&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: function_output, &quot;tool_call_id&quot;: completion.choices[0].message.tool_calls[0].id&#125;)print(&quot;已添加tool message\\n&quot;)completion = function_calling() WorkFlow - 编排的Agent上面提到，对于复杂任务，完全让大模型去规划，去自由探索，可能出现很多不可预知的问题，这对于一些确定性的任务来说，是很致命的，既然这样，那么就别让大模型逞能，脚踏实地，只解决具体的单步问题，规划问题让人类来干就好了，这就是workflow，这就是截止到2025年4月20日来最主流的Agent 实现方式； MCP上面是通义千问的Function 调用方式，下次我们再添加一个新的工具的时候，是不是又要重复写一下这个流程(定义工具，定义工具描述，然后传给大模型)， 但是现在大模型这么多，大模型之间的Function Call的方式可能不相同，这里写的工具函数，换到GPT4，可能就要修改代码，这样很不符合程序员的复用规则，因此有了MCP MCP其实还是大模型选择需要调用的工具，因此MCP本质还是Function Call， 只是统一了一个标准之后，开发者通过MCP协议写的Function可以给别人复用了 如果想深入了解的话，大家可以对照着官方代码写一遍，就会发现了MCP解决的是哪一方面的问题了； A2A，ANP等等A2A：谷歌提出来的一种 Agent 和 Agent相互通信的协议， MCP协议统一了大模型与外界工具交互的方式，A2A是Agent与Agent间的，可以类比成公司里面部门，每个部门承担一部功能(如前端，后端，数据，算法，HR，财务，运维等)，然后相互协作，共同达成一个目标； ANP：国内提出的类似于A2A的，用于Agent2Agent的协议，目前是成为互联网界的http . . . . . 无论是MCP，还是A2A，抑或者是其它协议，我觉得归根到底是今年自deepseek以来，**大模型的能力得到了较大的提升，**这个生态才慢慢火起来(2024年开年火了一阵就没声音了，因为Agent有点人工智障的味道)，至于Agent有多智能，我觉得还是让子弹飞一会儿，但是在小的方面，确实会影响我们的工作效率；恰当的使用llm来协助我们工作，能做到事半功倍； 好物推荐编程工具类： Cursor (目前最好的AI代码编辑器，就是有点小贵，我使用的是VScode通义灵码插件替代) Trae (字节出品， 国产第一个有知名度的AI代码编辑器(当然，投流，广告投入很大)) 开发框架类:（还是上面那两个） langchain：https://github.com/langchain-ai/langchain llamaindex：https://github.com/run-llama/llama_index (个人常用这个，教程很友好， 但是目前langchain发展更好一些) 零代码类大模型编排工具(适合AI行业的所有人) 搞过comfyUI的同学应该知道节点编辑工具的概念，这个就是llm这边的节点编辑工具 dify : 企业级的，目前大多数工具开发AI流程应用的首选 n8n ：同上，优势在集成了很多的工具 扣子 ： 字节出品，和dify类似 举个dify的例子： 本来想演示一个使用dify搭建小影知识库问答系统的demo，但是貌似飞书文档权限比较严格&#x3D;_&#x3D;#，因此这里简单介绍一下； 以翻译的任务举个例子(目前主流翻译的流程)：通过搭积木的方式，20分钟(2分钟搭建，18分钟的提示词) 即可快速完成一个任务的搭建 另外，比较喜欢的一个功能是不同模型的对比，我觉得在验证阶段很实用 现场演示Agentcline + MCP MCP发展得如火如荼，开源社区上也有很多好玩的MCP工具了，大家可以尝试一下 想一想：大家现在看到这个热点，大家觉得是通过什么实现的？ difyima (好用的结合RAG的文档工具)","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"LLM","slug":"LLM","permalink":"https://flippy-bird.github.io/tags/LLM/"}]},{"title":"RAG优化","slug":"RAG优化","date":"2025-04-21T08:10:12.000Z","updated":"2025-11-28T08:41:39.772Z","comments":true,"path":"2025/04/21/RAG优化/","permalink":"https://flippy-bird.github.io/2025/04/21/RAG%E4%BC%98%E5%8C%96/","excerpt":"","text":"1. 基本的RAG流程-Native RAG 2. RAG优化2.1 优化chunk传统的RAG在数据分块时采用的是固定分块，因为可能会破坏段落的连续性，因此这一方面可以改进 2.1.1 拼接断裂的块 将固定分块的断裂部分找到，然后拼接起来； 123456789101112131415161718192021222324252627282930313233def compute_breakpoints(similarities, method=&quot;percentile&quot;, threshold=90): &quot;&quot;&quot; Computes chunking breakpoints based on similarity drops. Args: similarities (List[float]): List of similarity scores between sentences. method (str): &#x27;percentile&#x27;, &#x27;standard_deviation&#x27;, or &#x27;interquartile&#x27;. threshold (float): Threshold value (percentile for &#x27;percentile&#x27;, std devs for &#x27;standard_deviation&#x27;). Returns: List[int]: Indices where chunk splits should occur. &quot;&quot;&quot; # Determine the threshold value based on the selected method if method == &quot;percentile&quot;: # Calculate the Xth percentile of the similarity scores threshold_value = np.percentile(similarities, threshold) elif method == &quot;standard_deviation&quot;: # Calculate the mean and standard deviation of the similarity scores mean = np.mean(similarities) std_dev = np.std(similarities) # Set the threshold value to mean minus X standard deviations threshold_value = mean - (threshold * std_dev) elif method == &quot;interquartile&quot;: # Calculate the first and third quartiles (Q1 and Q3) q1, q3 = np.percentile(similarities, [25, 75]) # Set the threshold value using the IQR rule for outliers threshold_value = q1 - 1.5 * (q3 - q1) else: # Raise an error if an invalid method is provided raise ValueError(&quot;Invalid method. Choose &#x27;percentile&#x27;, &#x27;standard_deviation&#x27;, or &#x27;interquartile&#x27;.&quot;) # Identify indices where similarity drops below the threshold value return [i for i, sim in enumerate(similarities) if sim &lt; threshold_value] 2.1.2 使用不同的分块大小 (例如原来是256， 现在试一试512， 1000等等)； 2.1.3 总结chunk chunk Embedding之后缺乏语义信息，在chunk Embedding之前 给每一个chunk添加一个meta data, 一般是对这段text的总结，如标题等(可以使用LLM来协助), 在查询时， 使用的是两个信息相似度的平均值 （这个demo） 1234567891011121314151617181920212223242526272829303132333435system_prompt = &quot;Generate a concise and informative title for the given text.&quot;# **************************** step 2 *********************************chunks = [] # Initialize an empty list to store chunks# Iterate through the text with the specified chunk size and overlapfor i in range(0, len(text), n - overlap): chunk = text[i:i + n] # Extract a chunk of text header = generate_chunk_header(chunk) # Generate a header for the chunk using LLM chunks.append(&#123;&quot;header&quot;: header, &quot;text&quot;: chunk&#125;) # Append the header and chunk to the listreturn chunks # Return the list of chunks with headers# *************************** step 3 **********************************for chunk in tqdm(text_chunks, desc=&quot;Generating embeddings&quot;): # Create an embedding for the chunk&#x27;s text text_embedding = create_embeddings(chunk[&quot;text&quot;]) # Create an embedding for the chunk&#x27;s header header_embedding = create_embeddings(chunk[&quot;header&quot;]) # Append the chunk&#x27;s header, text, and their embeddings to the list embeddings.append(&#123;&quot;header&quot;: chunk[&quot;header&quot;], &quot;text&quot;: chunk[&quot;text&quot;], &quot;embedding&quot;: text_embedding, &quot;header_embedding&quot;: header_embedding&#125;) # ****************************** step 4**************************************# Iterate through each chunk to calculate similarity scoresfor chunk in chunks: # Compute cosine similarity between query embedding and chunk text embedding sim_text = cosine_similarity(np.array(query_embedding), np.array(chunk[&quot;embedding&quot;])) # Compute cosine similarity between query embedding and chunk header embedding sim_header = cosine_similarity(np.array(query_embedding), np.array(chunk[&quot;header_embedding&quot;])) # Calculate the average similarity score avg_similarity = (sim_text + sim_header) / 2 # Append the chunk and its average similarity score to the list similarities.append((chunk, avg_similarity)) 2.1.4 提问chunk 同上，不过这里改变了方向，是从chunk中提取问题，而不是总结，换汤不换药， 注意这里的是 问题 + chunk text 向量化之后一起 进行检索，然后找出top-k 1234567891011# Define the system prompt to guide the AI&#x27;s behaviorsystem_prompt = &quot;You are an expert at generating relevant questions from text. Create concise questions that can be answered using only the provided text. Focus on key information and concepts.&quot;# Define the user prompt with the text chunk and the number of questions to generateuser_prompt = f&quot;&quot;&quot;Based on the following text, generate &#123;num_questions&#125; different questions that can be answered using only this text:&#123;text_chunk&#125;Format your response as a numbered list of questions only, with no additional text.&quot;&quot;&quot; 2.1.5 提取chunk，不采用固定chunk size Proposition Chunking 仅仅采用固定分块，问题太多，会破坏句子的语义，句子不完整时会产生歧义等, 因此这种方法是从每个chunk中提取有用的关键信息作为新的chunk (这里也是使用LLM来分句的) 主要是两步：分段 然后评估筛选出最符合的 123456789101112131415161718192021222324252627282930313233############### 产生新的chunk# System prompt to instruct the AI on how to generate propositionssystem_prompt = &quot;&quot;&quot;Please break down the following text into simple, self-contained propositions. Ensure that each proposition meets the following criteria:1. Express a Single Fact: Each proposition should state one specific fact or claim.2. Be Understandable Without Context: The proposition should be self-contained, meaning it can be understood without needing additional context.3. Use Full Names, Not Pronouns: Avoid pronouns or ambiguous references; use full entity names.4. Include Relevant Dates/Qualifiers: If applicable, include necessary dates, times, and qualifiers to make the fact precise.5. Contain One Subject-Predicate Relationship: Focus on a single subject and its corresponding action or attribute, without conjunctions or multiple clauses.Output ONLY the list of propositions without any additional text or explanations.&quot;&quot;&quot;# User prompt containing the text chunk to be converted into propositionsuser_prompt = f&quot;Text to convert into propositions:\\n\\n&#123;chunk[&#x27;text&#x27;]&#125;&quot;################# 评估新chunk的质量 (结合原始的chunk)# System prompt to instruct the AI on how to evaluate the propositionsystem_prompt = &quot;&quot;&quot;You are an expert at evaluating the quality of propositions extracted from text.Rate the given proposition on the following criteria (scale 1-10):- Accuracy: How well the proposition reflects information in the original text- Clarity: How easy it is to understand the proposition without additional context- Completeness: Whether the proposition includes necessary details (dates, qualifiers, etc.)- Conciseness: Whether the proposition is concise without losing important informationThe response must be in valid JSON format with numerical scores for each criterion:&#123;&quot;accuracy&quot;: X, &quot;clarity&quot;: X, &quot;completeness&quot;: X, &quot;conciseness&quot;: X&#125;&quot;&quot;&quot;# User prompt containing the proposition and the original textuser_prompt = f&quot;&quot;&quot;Proposition: &#123;proposition&#125; 2.2 优化处理过程2.2.1 分层RAG传统的RAG当 text很大时，chunk就很多，匹配chunk缺乏上下文，而且每一个chunk都做相似度计算，计算量大，效率低，因此分层RAG的思想是，先对每一页做一个总结，先在总结中找到相关信息，然后再在对应的页中去找相关的详细信息 1234567############## 总结的prompt# Define the system prompt to instruct the summarization modelsystem_prompt = &quot;&quot;&quot;You are an expert summarization system.Create a detailed summary of the provided text. Focus on capturing the main topics, key information, and important facts.Your summary should be comprehensive enough to understand what the page containsbut more concise than the original.&quot;&quot;&quot; 2.2.2 GraphRAG可参考：GraphRAG快速入门与原理详解 实体提取 (Node, 节点)：利用LLM进行实体的提取，这些实体通常是指文档中出现的人物、地点、组织、概念等信息。 关系提取(Edage， 边)：关系挖掘是从文本中识别出实体之间的 关系，例如：谁与谁有关联、某个实体与另一个实体之间的关系是“属于”、“合作”、“对立”等。 Community(中文翻译成社区，感觉真的很奇怪，直接使用英文就好) 构建： 感觉是为了将不同部分的信息联系起来 通过使用 图谱聚类算法（如 Leiden 算法），GraphRAG 会将不同的实体和关系分组，形成多个 社区（Community）。这些社区是根据实体之间的相似度或关系的密切程度进行划分的。这种分组帮助 GraphRAG 更好地理解不同知识领域的结构， 2.2.3 KAGhttps://github.com/OpenSPG/KAG/blob/master/README_cn.md 2.3 后处理由于固定分块会破坏句子的连续性 2.3.1 增加相邻上下文 在检索这一步时，将检索到的相邻chunk也包含进来以增加上下文，减少信息的损失 2.3.2 ReRank ReRank 就是觉得通过计算余弦相似度的方式得到的相似度具有一定的可信度，但是不高，因此使用其它方法对得到的top-k进行重排序，然后选择相关度最高的几个答案(一般使用LLM来实现) 12345678910system_prompt = &quot;&quot;&quot;You are an expert at evaluating document relevance for search queries.Your task is to rate documents on a scale from 0 to 10 based on how well they answer the given query.Guidelines:- Score 0-2: Document is completely irrelevant- Score 3-5: Document has some relevant information but doesn&#x27;t directly answer the query- Score 6-8: Document is relevant and partially answers the query- Score 9-10: Document is highly relevant and directly answers the queryYou MUST respond with ONLY a single integer score between 0 and 10. Do not include ANY other text.&quot;&quot;&quot; 也可以使用关键词的方式 1234567891011121314151617181920212223242526272829# Extract important keywords from the querykeywords = [word.lower() for word in query.split() if len(word) &gt; 3]scored_results = [] # Initialize a list to store scored resultsfor result in results: document_text = result[&quot;text&quot;].lower() # Convert document text to lowercase # Base score starts with vector similarity base_score = result[&quot;similarity&quot;] * 0.5 # Initialize keyword score keyword_score = 0 for keyword in keywords: if keyword in document_text: # Add points for each keyword found keyword_score += 0.1 # Add more points if keyword appears near the beginning first_position = document_text.find(keyword) if first_position &lt; len(document_text) / 4: # In the first quarter of the text keyword_score += 0.1 # Add points for keyword frequency frequency = document_text.count(keyword) keyword_score += min(0.05 * frequency, 0.2) # Cap at 0.2 # Calculate the final score by combining base score and keyword score final_score = base_score + keyword_score 2.3.3 增加上下文-v2 计算连续chunk (分段)的总相关性，然后输出 top-k 个分段 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2): &quot;&quot;&quot; Find the best segments using a variant of the maximum sum subarray algorithm. Args: chunk_values (List[float]): Values for each chunk max_segment_length (int): Maximum length of a single segment total_max_length (int): Maximum total length across all segments min_segment_value (float): Minimum value for a segment to be considered Returns: List[Tuple[int, int]]: List of (start, end) indices for best segments &quot;&quot;&quot; print(&quot;Finding optimal continuous text segments...&quot;) best_segments = [] segment_scores = [] total_included_chunks = 0 # Keep finding segments until we hit our limits while total_included_chunks &lt; total_max_length: best_score = min_segment_value # Minimum threshold for a segment best_segment = None # Try each possible starting position for start in range(len(chunk_values)): # Skip if this start position is already in a selected segment if any(start &gt;= s[0] and start &lt; s[1] for s in best_segments): continue # Try each possible segment length for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1): end = start + length # Skip if end position is already in a selected segment if any(end &gt; s[0] and end &lt;= s[1] for s in best_segments): continue # Calculate segment value as sum of chunk values segment_value = sum(chunk_values[start:end]) # Update best segment if this one is better if segment_value &gt; best_score: best_score = segment_value best_segment = (start, end) # If we found a good segment, add it if best_segment: best_segments.append(best_segment) segment_scores.append(best_score) total_included_chunks += best_segment[1] - best_segment[0] print(f&quot;Found segment &#123;best_segment&#125; with score &#123;best_score:.4f&#125;&quot;) else: # No more good segments to find break # Sort segments by their starting position for readability best_segments = sorted(best_segments, key=lambda x: x[0]) return best_segments, segment_scores 2.3.4 使用LLM过滤无效信息 使用LLM对top-k的chunk text进行判断，过滤，然后帮助我们筛选出最符合的信息，下面是实现的三种方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445if compression_type == &quot;selective&quot;: system_prompt = &quot;&quot;&quot;You are an expert at information filtering. Your task is to analyze a document chunk and extract ONLY the sentences or paragraphs that are directly relevant to the user&#x27;s query. Remove all irrelevant content. Your output should: 1. ONLY include text that helps answer the query 2. Preserve the exact wording of relevant sentences (do not paraphrase) 3. Maintain the original order of the text 4. Include ALL relevant content, even if it seems redundant 5. EXCLUDE any text that isn&#x27;t relevant to the query Format your response as plain text with no additional comments.&quot;&quot;&quot;elif compression_type == &quot;summary&quot;: system_prompt = &quot;&quot;&quot;You are an expert at summarization. Your task is to create a concise summary of the provided chunk that focuses ONLY on information relevant to the user&#x27;s query. Your output should: 1. Be brief but comprehensive regarding query-relevant information 2. Focus exclusively on information related to the query 3. Omit irrelevant details 4. Be written in a neutral, factual tone Format your response as plain text with no additional comments.&quot;&quot;&quot;else: # extraction system_prompt = &quot;&quot;&quot;You are an expert at information extraction. Your task is to extract ONLY the exact sentences from the document chunk that contain information relevant to answering the user&#x27;s query. Your output should: 1. Include ONLY direct quotes of relevant sentences from the original text 2. Preserve the original wording (do not modify the text) 3. Include ONLY sentences that directly relate to the query 4. Separate extracted sentences with newlines 5. Do not add any commentary or additional text Format your response as plain text with no additional comments.&quot;&quot;&quot;# Define the user prompt with the query and document chunkuser_prompt = f&quot;&quot;&quot; Query: &#123;query&#125; Document Chunk: &#123;chunk&#125; Extract only the content relevant to answering this query.&quot;&quot;&quot; 2.3.5 优化检索策略- Adaptive Retrieval本质还是query的改写 RAG的效果差，也有可能是检索的策略有问题，对于所有的问题，全部采用向量相似度匹配，可能不太对，因此针对不同的用户问题，需要采取不同的策略 对查询类型进行分类: 事实、分析、观点或情境 （使用大模型来进行选择） 然后 Query Transform （根据上面的类型进行改写）, 然后和之前一样进行相似度进行匹配 12345678910# Define the system prompt to guide the AI&#x27;s classificationsystem_prompt = &quot;&quot;&quot;You are an expert at classifying questions. Classify the given query into exactly one of these categories: - Factual: Queries seeking specific, verifiable information. - Analytical: Queries requiring comprehensive analysis or explanation. - Opinion: Queries about subjective matters or seeking diverse viewpoints. - Contextual: Queries that depend on user-specific context. Return ONLY the category name, without any explanation or additional text.&quot;&quot;&quot; query 改写的部分 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131##################### 基于事实 Factual system_prompt = &quot;&quot;&quot;You are an expert at enhancing search queries. Your task is to reformulate the given factual query to make it more precise and specific for information retrieval. Focus on key entities and their relationships. Provide ONLY the enhanced query without any explanation.&quot;&quot;&quot;user_prompt = f&quot;Enhance this factual query: &#123;query&#125;&quot;##################### 基于事实 Factual system_prompt = &quot;&quot;&quot;You are an expert at enhancing search queries. Your task is to reformulate the given factual query to make it more precise and specific for information retrieval. Focus on key entities and their relationships. Provide ONLY the enhanced query without any explanation.&quot;&quot;&quot;user_prompt = f&quot;Enhance this factual query: &#123;query&#125;&quot;### 基于分析 Analytical # Define the system prompt to guide the AI in generating sub-questionssystem_prompt = &quot;&quot;&quot;You are an expert at breaking down complex questions.Generate sub-questions that explore different aspects of the main analytical query.These sub-questions should cover the breadth of the topic and help retrieve comprehensive information.Return a list of exactly 3 sub-questions, one per line.&quot;&quot;&quot;# Create the user prompt with the main queryuser_prompt = f&quot;Generate sub-questions for this analytical query: &#123;query&#125;&quot;################## 基于分析 Analytical # Define the system prompt to guide the AI in generating sub-questionssystem_prompt = &quot;&quot;&quot;You are an expert at breaking down complex questions.Generate sub-questions that explore different aspects of the main analytical query.These sub-questions should cover the breadth of the topic and help retrieve comprehensive information.Return a list of exactly 3 sub-questions, one per line.&quot;&quot;&quot;# Create the user prompt with the main queryuser_prompt = f&quot;Generate sub-questions for this analytical query: &#123;query&#125;&quot;################## 基于观点 Opinion Strategy # Define the system prompt to guide the AI in identifying different perspectivessystem_prompt = &quot;&quot;&quot;You are an expert at identifying different perspectives on a topic. For the given query about opinions or viewpoints, identify different perspectives that people might have on this topic. Return a list of exactly 3 different viewpoint angles, one per line.&quot;&quot;&quot;# Create the user prompt with the main queryuser_prompt = f&quot;Identify different perspectives on: &#123;query&#125;&quot;# Extract and clean the viewpointsviewpoints = response.choices[0].message.content.strip().split(&#x27;\\n&#x27;)viewpoints = [v.strip() for v in viewpoints if v.strip()]print(f&quot;Identified viewpoints: &#123;viewpoints&#125;&quot;)# Retrieve documents representing each viewpointall_results = []for viewpoint in viewpoints: # 注意这里的做法 # Combine the main query with the viewpoint combined_query = f&quot;&#123;query&#125; &#123;viewpoint&#125;&quot; # Create embeddings for the combined query viewpoint_embedding = create_embeddings(combined_query) # Perform similarity search for the combined query results = vector_store.similarity_search(viewpoint_embedding, k=2) # Mark results with the viewpoint they represent for result in results: result[&quot;viewpoint&quot;] = viewpoint # Add the results to the list of all results all_results.extend(results) ################### 基于情境 Contextual Strategyif not user_context:system_prompt = &quot;&quot;&quot;You are an expert at understanding implied context in questions.For the given query, infer what contextual information might be relevant or implied but not explicitly stated. Focus on what background would help answering this query.Return a brief description of the implied context.&quot;&quot;&quot;user_prompt = f&quot;Infer the implied context in this query: &#123;query&#125;&quot;# Generate the inferred context using the LLMresponse = client.chat.completions.create( model=&quot;meta-llama/Llama-3.2-3B-Instruct&quot;, messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_prompt&#125; ], temperature=0.1)# Extract and print the inferred contextuser_context = response.choices[0].message.content.strip()print(f&quot;Inferred context: &#123;user_context&#125;&quot;)# Reformulate the query to incorporate contextsystem_prompt = &quot;&quot;&quot;You are an expert at reformulating questions with context.Given a query and some contextual information, create a more specific query that incorporates the context to get more relevant information.Return ONLY the reformulated query without explanation.&quot;&quot;&quot;user_prompt = f&quot;&quot;&quot;Query: &#123;query&#125;Context: &#123;user_context&#125;Reformulate the query to incorporate this context:&quot;&quot;&quot;# Generate the contextualized query using the LLMresponse = client.chat.completions.create(model=&quot;meta-llama/Llama-3.2-3B-Instruct&quot;,messages=[ &#123;&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: system_prompt&#125;, &#123;&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: user_prompt&#125;],temperature=0) 2.3.6 CRAG对检索的文件进行了修正 校验检索得到的文档，如果文档相关性评分比较高(相关性分数还是使用LLM)，那么继续原先的步骤即可；如果相关性评分很低，那么采用外部的工具(如浏览器等)来搜索更多的资料，用以补充；如果相关性评分在中间，结合检索文本和搜索文本 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677def crag_process(query, vector_store, k=3): print(f&quot;\\n=== Processing query with CRAG: &#123;query&#125; ===\\n&quot;) # Step 1: Create query embedding and retrieve documents print(&quot;Retrieving initial documents...&quot;) query_embedding = create_embeddings(query) retrieved_docs = vector_store.similarity_search(query_embedding, k=k) # Step 2: Evaluate document relevance print(&quot;Evaluating document relevance...&quot;) relevance_scores = [] for doc in retrieved_docs: score = evaluate_document_relevance(query, doc[&quot;text&quot;]) relevance_scores.append(score) doc[&quot;relevance&quot;] = score print(f&quot;Document scored &#123;score:.2f&#125; relevance&quot;) # Step 3: Determine action based on best relevance score max_score = max(relevance_scores) if relevance_scores else 0 best_doc_idx = relevance_scores.index(max_score) if relevance_scores else -1 # Track sources for attribution sources = [] final_knowledge = &quot;&quot; # Step 4: Execute the appropriate knowledge acquisition strategy if max_score &gt; 0.7: # Case 1: High relevance - Use document directly print(f&quot;High relevance (&#123;max_score:.2f&#125;) - Using document directly&quot;) best_doc = retrieved_docs[best_doc_idx][&quot;text&quot;] final_knowledge = best_doc sources.append(&#123; &quot;title&quot;: &quot;Document&quot;, &quot;url&quot;: &quot;&quot; &#125;) elif max_score &lt; 0.3: # Case 2: Low relevance - Use web search print(f&quot;Low relevance (&#123;max_score:.2f&#125;) - Performing web search&quot;) web_results, web_sources = perform_web_search(query) final_knowledge = refine_knowledge(web_results) sources.extend(web_sources) else: # Case 3: Medium relevance - Combine document with web search print(f&quot;Medium relevance (&#123;max_score:.2f&#125;) - Combining document with web search&quot;) best_doc = retrieved_docs[best_doc_idx][&quot;text&quot;] refined_doc = refine_knowledge(best_doc) # Get web results web_results, web_sources = perform_web_search(query) refined_web = refine_knowledge(web_results) # Combine knowledge final_knowledge = f&quot;From document:\\n&#123;refined_doc&#125;\\n\\nFrom web search:\\n&#123;refined_web&#125;&quot; # Add sources sources.append(&#123; &quot;title&quot;: &quot;Document&quot;, &quot;url&quot;: &quot;&quot; &#125;) sources.extend(web_sources) # Step 5: Generate final response print(&quot;Generating final response...&quot;) response = generate_response(query, final_knowledge, sources) # Return comprehensive results return &#123; &quot;query&quot;: query, &quot;response&quot;: response, &quot;retrieved_docs&quot;: retrieved_docs, &quot;relevance_scores&quot;: relevance_scores, &quot;max_relevance&quot;: max_score, &quot;final_knowledge&quot;: final_knowledge, &quot;sources&quot;: sources &#125; 2.4 输入部分2.4.1 Query Transform Query改写 可能是觉得用户提的问题不够具体，导致搜索的时候找到对应的信息；亦或者是用户的问题太复杂了，包含了很多层级的问题；亦或者是用户的问题太细致了，以至匹配不到相关的信息，需要对信息step back； 12345678910111213141516171819202122232425262728293031323334353637383940414243############ 1.Query Rewriting # Define the system prompt to guide the AI assistant&#x27;s behaviorsystem_prompt = &quot;You are an AI assistant specialized in improving search queries. Your task is to rewrite user queries to be more specific, detailed, and likely to retrieve relevant information.&quot;# Define the user prompt with the original query to be rewrittenuser_prompt = f&quot;&quot;&quot;Rewrite the following query to make it more specific and detailed. Include relevant terms and concepts that might help in retrieving accurate information.Original query: &#123;original_query&#125;Rewritten query:&quot;&quot;&quot;############ 2. Step-back Prompting# Define the system prompt to guide the AI assistant&#x27;s behaviorsystem_prompt = &quot;You are an AI assistant specialized in search strategies. Your task is to generate broader, more general versions of specific queries to retrieve relevant background information.&quot;# Define the user prompt with the original query to be generalizeduser_prompt = f&quot;&quot;&quot;Generate a broader, more general version of the following query that could help retrieve useful background information.Original query: &#123;original_query&#125;Step-back query:&quot;&quot;&quot;############ 3. Sub-query Decomposition# Define the system prompt to guide the AI assistant&#x27;s behaviorsystem_prompt = &quot;You are an AI assistant specialized in breaking down complex questions. Your task is to decompose complex queries into simpler sub-questions that, when answered together, address the original query.&quot;# Define the user prompt with the original query to be decomposeduser_prompt = f&quot;&quot;&quot;Break down the following complex query into &#123;num_subqueries&#125; simpler sub-queries. Each sub-query should focus on a different aspect of the original question.Original query: &#123;original_query&#125;Generate &#123;num_subqueries&#125; sub-queries, one per line, in this format:1. [First sub-query]2. [Second sub-query]And so on...&quot;&quot;&quot; 2.4.2 HyDE (Hypothetical Document Embedding) 本质上还是Query改写的一种，具体操作方法如下： 根据用户的问题，假想一个document，这个document可以解决用户的query； (这里使用的还是LLM) 然后使用这个document进行向量化和相似度的比较，继续后面的流程； 12345678910# Define the system prompt to instruct the model on how to generate the documentsystem_prompt = f&quot;&quot;&quot;You are an expert document creator. Given a question, generate a detailed document that would directly answer this question.The document should be approximately &#123;desired_length&#125; characters long and provide an in-depth, informative answer to the question. Write as if this document is from an authoritative sourceon the subject. Include specific details, facts, and explanations.Do not mention that this is a hypothetical document - just write the content directly.&quot;&quot;&quot;# Define the user prompt with the queryuser_prompt = f&quot;Question: &#123;query&#125;\\n\\nGenerate a document that fully answers this question:&quot;","categories":[],"tags":[{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"}]},{"title":"GraphRAG","slug":"GraphRAG","date":"2025-03-13T02:14:50.000Z","updated":"2025-11-28T08:40:43.787Z","comments":true,"path":"2025/03/13/GraphRAG/","permalink":"https://flippy-bird.github.io/2025/03/13/GraphRAG/","excerpt":"","text":"1. why we need GraphRAG? https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/ 基本的RAG 难以建立信息关联。当回答某个问题需要遍历不同信息片段并通过其共享属性来提供新的综合见解时，就会出现这种情况。 基本RAG 在被要求整体理解大型数据集合甚至单个大型文档中的汇总语义概念时表现不佳。 我想到的一个例子是：假设我有一个介绍 LLM的文章，然后分段式介绍了LLM的一些特点(假设每一段不显式的包含LLM信息，用它，他或者其他代词指代)，然后这些信息被分别分块，然后embedding，在query (问LLM的优点， 需要根据LLM的特点来总结)，search的时候，应该就不会查到这些信息。 2. GraphRAG的原理2.1 构建阶段在建立索引(index)这一步，主要按照下面的步骤进行： 将输入语料库分割为一系列的文本单元（TextUnits），这些单元作为处理以下步骤的可分析单元，并在我们的输出中提供细粒度的引用。 使用 LLM 从文本单元中提取所有实体、关系和关键声明。并且对每个实体，关系，文本生成embedding向量 使用 Leiden 技术 对知识图谱进行层次聚类。 GraphRAG通过计算实体之间的关系，填充关系表，并生成关于实体的社区报告来总结不同实体之间的关系与上下文，自下而上地生成每个社区层级及其组成部分的摘要。这有助于对数据集的整体理解。 具体的例子可以参考这篇知乎文章的例子: GraphRAG快速入门与原理详解 2.1 查询阶段在查询这一步，又分为两种方式： 全局搜索，用于通过利用社区层级摘要来推理有关语料库的整体问题。 局部搜索，用于通过扩展到其邻居和相关概念来推理特定实体的情况。 局部搜索 有点类似BM25关键词搜索，然后找到一些相关的实体关系之后，然后再利用图谱的节点关系再在领近查找更多信息，然后再汇总 全局搜索 根据用户的问题，全局搜索会搜索相关的社区报告 ，并且给每一个社区报告打分(使用LLM来进行)，根据打分的高低，然后将最相关的社区报告给到大模型的上下文中； 3. 项目源码学习 TODO","categories":[{"name":"RAG框架学习","slug":"RAG框架学习","permalink":"https://flippy-bird.github.io/categories/RAG%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"}]},{"title":"AR的前世今生","slug":"AR的前世今生","date":"2022-09-27T07:58:23.000Z","updated":"2025-11-27T10:11:48.582Z","comments":true,"path":"2022/09/27/AR的前世今生/","permalink":"https://flippy-bird.github.io/2022/09/27/AR%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","excerpt":"","text":"1.概念区分什么是AR&#x2F;VR&#x2F;MR&#x2F;XR? 在前段时间，元宇宙大火的时候，你应该听过或者见过上面的几个词语。什么？元宇宙都没有听过，建议食用下面的资料了解一下,也是蛮有意思的东西。 聊聊这个本不存在的“元宇宙” 啥？全是文字，没事，那就看个视频吧，你也可以简单了解一下这篇文章的主题AR是个什么东西！ 2021年了，AR眼镜可以做什么？ AR 对现实的一种增强，设备对现实识别（形状、位置、动作、边缘）,从而将虚拟信息叠加在现实中。 VR 为用户提供一个虚拟的世界，不必考虑现实世界，是一种沉浸式的体验。如果喜欢看动漫的，**动漫《刀剑神域》里描述就是这样一个世界(PS: 我感觉这就是当前阶段元宇宙的最终形态)；在电影《头号玩家》**中，里面塑造的绿洲也是这样的一个虚拟世界。 MR 虚拟世界和现实世界的混合。 AR、VR、MR统称XR。VR的发展相较于AR的发展，更为成熟一些，有很多VR面对用户的产品，例如Pico 4，这段时间发布了它的新产品。在这里，我们主要聊一聊AR相关的内容。 2.AR历史发展及现状1992年波音公司研究员提出了AR这个概念，自此AR诞生了。 2.1 AR序幕的开启把时间拉回10年前的2012年，在一个慈善晚宴上，谷歌联合创始人谢尔盖·布林为了展示一把谷歌的技术实力，就把尚在原型阶段的谷歌智能眼镜Google Glass掏了出来，众人直呼“卧槽”，随后便引起了大众的广泛讨论和关注。正当大家做着钢铁侠梦的时候，也遇到了视频中小何同学遇到的各种使用问题；虽然Google Glasss更多意义上只是一个“戴在脸上的手机”，很多AR领域涉及的技术，受限于当时的技术和算力，诸如环境检测，用户交互技术等并不涉及，但是其展现的前景还是值得大家期待的。因此，可以说，Google Glass拉起了AR快速发展的序幕。 2.2 AR的持续发展 AR界的第一代黑科技 自从谷歌将Google Glass引入大众的视野，旁边的微软可坐不住了，论技术，咱可没输过谁，就算打不过，那不是最后还可以加入嘛。况且，你的那个Google Glass也太垃圾了吧，这么重，还这么贵，也就是你是第一人，否则不会有那么多人买账，让我来教教你怎么制作一个好的产品吧，是时候展现真正的技术了。于是，在2015年的时候，微软发布了Hololens，震惊了世界，它能检测外界环境，将虚拟物体和显示环境融合，并且它通过追踪用户手势来完成交互。HoloLens2 是真正意义上的第一款相对成熟的AR设备，它的出现，引领了AR的潮流，成为了业界的标杆和对比的对象。 2019年，微软发布了HoloLens2，解决了视野狭小的问题，并且大幅度提升了手势追踪的准确性。虽然HoloLens2 虽然相对成熟，但是成本居高不下，**官方指导建议零售价是3500美元。**另外，虽然它相对体积较小，但仍重达500g, 已知iPhone 12重152g, 请问你的头上戴了几台iPhone 12 🤣 AR界的第一代黑”概念” 凭借着下面这“一条在体育馆地板跃起的大鲸鱼”，Magic Leap 成功塑造了一个 AR 黑科技公司的形象，以及足够激动人心的未来——从移动计算迁移到空间计算。裸眼AR的概念太具有震撼力，致使Magic Leap在视频发布的短短三个月内，迅速获得16亿美元融资，最高估值达64亿美元。然而巧妇难为无米之炊，在关键技术上没有取得核心突破的Magic Leap在2017年推出的Magic Leap one，其表现与其吹牛的效果差得太离谱，消费者纷纷给出了差评，这也给这家公司贴上了“骗子”的标签。 在上面的视频被人戳破，是视频后期合成之后，这家公司在AR界没有惊起任何浪花，就在写这篇文稿的当天，我在其官网上发现，Magic Leap将发布Magic Leap 2，这回他能否力挽狂澜，拯救自己“骗子”的形象，还得看过硬的产品！ 国内AR的发展 国内的AR公司主要有酷派的Xview系列、亮亮视野、影目科技等，要说目前最火的，当属Nreal， 在今年的8月23日，Nreal推出了新品Nreal Air和Nreal X两款AR眼镜；发布首日，Nreal销售额突破了1200万。 在9月21日，国内最早成立的AR企业之一亮亮视野发布了一款量产的AR字幕眼镜，也是全球第一款重量小于80g的双目波导无线一体机AR眼镜：听语者。主打听障者沟通。 国内的AR产业也在快速发展中，苹果老早就说2022年要发布AR眼镜，从年头等到了年尾，毛都没有；或许，我们可以把目光转向国内，期待国内的厂商能够发布一款改变世界的产品，就像当时iphone的出现一样。 2.3 移动端AR比较流行的SDK目前移动端AR SDK有很多，下面列出一些比较有影响力的SDK 苹果的ARKit ： 2017年发布至今，被认为是性能最强，最具商业潜力的sdk。 谷歌的ARCore ： 同时支持Android 和iOS, 是Android比较常用的sdk，综合实力比不上ARKit 历史悠久的 ARToolKit： 第一个AR SDK，但是目前开源社区不是太活跃 商业化sdk Vuforia： 性能表现很不错，支持Android、iOS 2.4 发展现状目前AR技术还有很多痛点，主要集中在以下几个方面： 检测环境的计算量很大，实时性不好，例如当你拿着设备快速移动时，虚拟物体和现实物体的融合就会出现飘移的现象 因为计算量大和频繁使用摄像头，耗电发热； 穿戴式AR 显示技术不成熟，大部分设备在清晰度、视野、体验性上都有些不足 软件生态不成熟 3.AR领域的关键技术3.1 探知环境的技术（自我定位、检测3D物体、环境建模） 没错，听到这些词语，很自然想到了自动驾驶里面也会有这样一个环节，就是”理解“环境的技术。 一般来说，建立环境地图需要经历以下的流程： 用户抵达一个新场所，开启摄像头。 设备根据陀螺仪等传感器确定自己的高度、朝向，并以此为空间坐标系原点。 识别用户摄像头输入，进行“点阵云”建模。 用户到处移动，直至整个地图被扫描建模完成。 3.2 显示技术AR的另一个关键技术就是显示技术，主要分为三类： 视觉差显示技术 全息投影技术 光影成像技术 3.3 交互技术目前为止，除了传统的触摸，按键、语音等技术，AR大致有3个方向的虚拟交互技术： 动作捕捉： 主要是手部动捕捉，目前Hololens2在这一块上技术积累最雄厚，不需要任何外设，直接通过手势就可以流畅操控设备。 眼动追踪： 使用摄像头捕捉人眼或脸部的图像，然后用算法实现人脸和人眼的检测、定位与跟踪，从而估算用户的视线变化。 脑机接口： 通过识别大脑活动电信号来操控设备，马斯克旗下的Neuralink公司通过向大脑植入十分之一发丝粗细的神经线，来检测和传输大脑的信号。 4.结语​ 20年前，当人们使用QICQ打着字，和天南海北的陌生人分享彼此的故事时，不会想到，今天我们一张图片，一个个表情包，一段视频，便可以轻松地将我们的故事分享给陌生人。20年前，我们省吃俭用抠抠搜搜计算着我们的2G流量，发个消息还老转圈圈，到今天，4G已悄然开启了全民直播时代，而5G也已在全面布局。 ​ 或许像电影《头号玩家》里面的元宇宙场景还需要很长一段路要走，但至少我觉得AR、VR技术将会在不久的将来带给我们全新的体验，就让我们且走，且看，且体验吧！","categories":[],"tags":[{"name":"元宇宙","slug":"元宇宙","permalink":"https://flippy-bird.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99/"},{"name":"AR","slug":"AR","permalink":"https://flippy-bird.github.io/tags/AR/"}]}],"categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"},{"name":"RAG框架学习","slug":"RAG框架学习","permalink":"https://flippy-bird.github.io/categories/RAG%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"plan","slug":"plan","permalink":"https://flippy-bird.github.io/tags/plan/"},{"name":"memory","slug":"memory","permalink":"https://flippy-bird.github.io/tags/memory/"},{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"},{"name":"MCP","slug":"MCP","permalink":"https://flippy-bird.github.io/tags/MCP/"},{"name":"LLM","slug":"LLM","permalink":"https://flippy-bird.github.io/tags/LLM/"},{"name":"元宇宙","slug":"元宇宙","permalink":"https://flippy-bird.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99/"},{"name":"AR","slug":"AR","permalink":"https://flippy-bird.github.io/tags/AR/"}]}
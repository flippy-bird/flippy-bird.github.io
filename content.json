{"meta":{"title":"大潘子的文字角落","subtitle":"","description":"","author":"Pan","url":"https://flippy-bird.github.io","root":"/"},"pages":[{"title":"categories","date":"2025-11-26T05:54:56.000Z","updated":"2025-11-26T05:55:08.826Z","comments":true,"path":"categories/index.html","permalink":"https://flippy-bird.github.io/categories/index.html","excerpt":"","text":""},{"title":"tags","date":"2025-11-26T05:53:10.000Z","updated":"2025-11-26T12:54:36.650Z","comments":true,"path":"tags/index.html","permalink":"https://flippy-bird.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"GraphRAG","slug":"GraphRAG","date":"2025-11-28T02:14:50.000Z","updated":"2025-11-28T06:20:31.965Z","comments":true,"path":"2025/11/28/GraphRAG/","permalink":"https://flippy-bird.github.io/2025/11/28/GraphRAG/","excerpt":"","text":"1. why we need GraphRAG? https://www.microsoft.com/en-us/research/blog/graphrag-unlocking-llm-discovery-on-narrative-private-data/ 基本的RAG 难以建立信息关联。当回答某个问题需要遍历不同信息片段并通过其共享属性来提供新的综合见解时，就会出现这种情况。 基本RAG 在被要求整体理解大型数据集合甚至单个大型文档中的汇总语义概念时表现不佳。 我想到的一个例子是：假设我有一个介绍 LLM的文章，然后分段式介绍了LLM的一些特点(假设每一段不显式的包含LLM信息，用它，他或者其他代词指代)，然后这些信息被分别分块，然后embedding，在query (问LLM的优点， 需要根据LLM的特点来总结)，search的时候，应该就不会查到这些信息。 2. GraphRAG的原理2.1 构建阶段在建立索引(index)这一步，主要按照下面的步骤进行： 将输入语料库分割为一系列的文本单元（TextUnits），这些单元作为处理以下步骤的可分析单元，并在我们的输出中提供细粒度的引用。 使用 LLM 从文本单元中提取所有实体、关系和关键声明。并且对每个实体，关系，文本生成embedding向量 使用 Leiden 技术 对知识图谱进行层次聚类。 GraphRAG通过计算实体之间的关系，填充关系表，并生成关于实体的社区报告来总结不同实体之间的关系与上下文，自下而上地生成每个社区层级及其组成部分的摘要。这有助于对数据集的整体理解。 具体的例子可以参考这篇知乎文章的例子: GraphRAG快速入门与原理详解 2.1 查询阶段在查询这一步，又分为两种方式： 全局搜索，用于通过利用社区层级摘要来推理有关语料库的整体问题。 局部搜索，用于通过扩展到其邻居和相关概念来推理特定实体的情况。 局部搜索 有点类似BM25关键词搜索，然后找到一些相关的实体关系之后，然后再利用图谱的节点关系再在领近查找更多信息，然后再汇总 全局搜索 根据用户的问题，全局搜索会搜索相关的社区报告 ，并且给每一个社区报告打分(使用LLM来进行)，根据打分的高低，然后将最相关的社区报告给到大模型的上下文中； 3. 项目源码学习 TODO","categories":[],"tags":[{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"}]},{"title":"AgentScope源码学习","slug":"AgentScope源码学习","date":"2025-11-25T11:47:53.000Z","updated":"2025-11-26T10:30:30.554Z","comments":true,"path":"2025/11/25/AgentScope源码学习/","permalink":"https://flippy-bird.github.io/2025/11/25/AgentScope%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"AgentScope (阿里的) https://github.com/agentscope-ai/agentscope 记忆长期记忆部分使用了mem0这个工具，当然，代码里面也提到了，可以使用阿里自家的ReMe这个记忆框架 Agent这一块儿使用的是基本的React模式，输出最后的回答，也成了一个工具；多了一个 1234567891011121314### AgentBase# 在AgentBase里面有一个虚函数 replydef reply()### 在ReactAgentBase里面有基本的React框架def _reasoning() # 虚函数def _acting() # 虚函数### 在ReactAgent里面def reply(): 这里就类似OpenManus里面的step()函数的作用了，从源码来看，逻辑完全一样 self._reasoning() ... self._acting() 在Agent里面有一个observe，感觉是为了观察到外界信息准备的接口(用于多Agent之间的信息互动) 1234567891011async def observe(self, msg: Msg | list[Msg] | None) -&gt; None: &quot;&quot;&quot;Receive the given message(s) without generating a reply. Args: msg (`Msg | list[Msg] | None`): The message(s) to be observed. &quot;&quot;&quot; raise NotImplementedError( f&quot;The observe function is not implemented in&quot; f&quot; &#123;self.__class__.__name__&#125; class.&quot;, ) 多Agent互动在这个框架里面使用的是swarm模式，似乎比较简单，每个agent observe其它agent的输出，添加到自己的记忆里面去就好了 在src&#x2F;pipeline&#x2F;_msghub.py文件夹里面 （或者见类名时的说明） 123456789async def broadcast(self, msg: list[Msg] | Msg) -&gt; None: &quot;&quot;&quot;Broadcast the message to all participants. Args: msg (`list[Msg] | Msg`): Message(s) to be broadcast among all participants. &quot;&quot;&quot; for agent in self.participants: await agent.observe(msg) Interrupt(中断介入)这个好像还不错，可以看一下 Plan的实现实现了一些plan的功能函数，然后让Agent去调用和更改当前的plan","categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"}]},{"title":"smolagents源码学习","slug":"smolagents源码学习","date":"2025-11-25T08:43:25.000Z","updated":"2025-11-26T12:53:27.394Z","comments":true,"path":"2025/11/25/smolagents源码学习/","permalink":"https://flippy-bird.github.io/2025/11/25/smolagents%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/","excerpt":"","text":"https://github.com/huggingface/smolagents memory 管理1234class AgentMemory: def __init__(self, system_prompt:str): self.system_prompt: SystemPromptStep = SystemPromptStep(system_prompt=system_prompt) self.steps: list[TaskStep | ActionStep | PlanningStep] = [] 将LLM执行过程的信息划分成了四个部分(主要，其它)： TaskStep: 与用户输入相关 (用户提问，上传图片等) SystemPromptStep: 系统的prompt PlanningStep: 与规划相关的记忆 (暂时还没遇到) ActionStep：当前送到LLM进行执行的信息 历史信息的获取 所以主要在于上面每种类型记忆数据 to_messages的实现 需要注意的是前后信息的完整性 1234567891011121314151617181920212223# agents.py line:1256def _step_stream( self, memory_step: ActionStep ) -&gt; Generator[ChatMessageStreamDelta | ToolCall | ToolOutput | ActionOutput]: &quot;&quot;&quot; Perform one step in the ReAct framework: the agent thinks, acts, and observes the result. Yields ChatMessageStreamDelta during the run if streaming is enabled. At the end, yields either None if the step is not final, or the final answer. &quot;&quot;&quot; memory_messages = self.write_memory_to_messages() # agents.py line:758def write_memory_to_messages( self, summary_mode: bool = False, ) -&gt; list[ChatMessage]: &quot;&quot;&quot; Reads past llm_outputs, actions, and observations or errors from the memory into a series of messages that can be used as input to the LLM. Adds a number of keywords (such as PLAN, error, etc) to help the LLM. &quot;&quot;&quot; messages = self.memory.system_prompt.to_messages(summary_mode=summary_mode) for memory_step in self.memory.steps: messages.extend(memory_step.to_messages(summary_mode=summary_mode)) return messages 边界处理当超过最大尝试次数（默认是20次）时，最后会总结19步的step，然后给出一个最终答案 12345678910111213141516# agent.py line:810def provide_final_answer(self, task: str) -&gt; ChatMessage: messages : 这里有一个专门针对这种情况的系统prompt messages += self.write_memory_to_messages()[1:] messages : 需要组装的post_messages try: chat_message: ChatMessage = self.model.generate(messages) return chat_message except Exception as e: return ChatMessage( role=MessageRole.ASSISTANT, content=[&#123;&quot;type&quot;: &quot;text&quot;, &quot;text&quot;: f&quot;Error in generating final LLM output: &#123;e&#125;&quot;&#125;], ) Agent: CodeAgent 项目的CodeAgent模式算是这个项目里面比较新颖的一种方式了，调用工具的API是通过执行python代码的方式来执行的，但是感觉解析python的AST那部分，就感觉好复杂-_-#，没有FunctionCall的这种方式简洁了。 来源于Executable Code Actions Elicit Better LLM Agents，这篇论文主要的出发动机是当前LLM Agent通常通过以预定义的格式生成 JSON 或文本来生成Action，这通常受到约束动作空间（例如，预定义工具的范围）和受限灵活性（例如，无法组合多个工具）的限制。 链接：https://zhuanlan.zhihu.com/p/16341067315","categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"}]},{"title":"Hello World","slug":"hello-world","date":"2025-11-25T07:59:06.847Z","updated":"2025-11-26T10:30:27.200Z","comments":true,"path":"2025/11/25/hello-world/","permalink":"https://flippy-bird.github.io/2025/11/25/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]},{"title":"MCP","slug":"mcp介绍","date":"2025-11-25T06:02:24.000Z","updated":"2025-11-25T08:40:56.342Z","comments":true,"path":"2025/11/25/mcp介绍/","permalink":"https://flippy-bird.github.io/2025/11/25/mcp%E4%BB%8B%E7%BB%8D/","excerpt":"","text":"MCP话不多说，直接上图即可 有MCP和没有MCP的区别，提升了效率 MCP的架构图：主要是由Host、Client和Server三部分组成 个人demo mcp demo 参考资料 MCP (Model Context Protocol)，一篇就够了。 python SDK的官方文档 测试MCP工具接入使用的地址：阿里MCP","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"MCP","slug":"MCP","permalink":"https://flippy-bird.github.io/tags/MCP/"}]},{"title":"大模型入门(组内分享)","slug":"大模型入门-组内分享","date":"2025-06-24T13:01:18.000Z","updated":"2025-11-28T02:05:03.478Z","comments":true,"path":"2025/06/24/大模型入门-组内分享/","permalink":"https://flippy-bird.github.io/2025/06/24/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E5%85%A5%E9%97%A8-%E7%BB%84%E5%86%85%E5%88%86%E4%BA%AB/","excerpt":"","text":"背景 2025年开年以来，大家或多或少都听过下面这些词：deepseek， AI， 人工智能，Agent，具身智能； AI到底发展到哪一步了，利用AI可以做哪些事情，或者作为技术人员，想要使用AI做一些事情，应该学习那些东西，这篇将和大家一起探讨下。 备注声明：该分享定位轻科普向，不会涉及大模型相关的底层原理，有些结论属于个人的理解和感悟，如需要深入交流，欢迎大家和我深入探讨。 大模型相关应用Prompt Engineer(提示词)Prompt 咒语 —-&gt; 激发LLM的潜能 2023年的某一天，当你开始和ChatGpt互动的那一刻开始，你就是一个Prompt Engineer了！ 当你不断试探GPT，使用各种策略，反问，试探，举个例子等等(专业一点就是few shot, zero-shot, cot等等)，最终得到正确答案时，你其实已经获得了一些prompt Engineer的训练，在下一次的问答中，你就能更快更好地得到你想要的答案； 当然啦，现在大模型也挺多的，确实有一些朴素的咒语框架可以让我们快速得到更好了的回答，你如果想了解一下，可以看看下面这个课程 (超爽中英!) 2025吴恩达最好的【提示词工程师】教程！附课件代码 DeepLearning.AI_大模型_LLM_Prompt_哔哩哔哩_bilibili 当然，上面说的咒语是GPT对话形式的，文生图，文生视频等等AIGC的应用，咒语可能需要另外习得(你芝麻开门可以开这个门，另外一个门可就不行了哦) SFT(微调) 大模型训练的时候使用的是公开的数据集，prompt咒语念得再好，也没办法，巧妇难为无米之炊，对于特定领域的问题大模型会出现幻觉(瞎回答) 大模型的效果出现一些问题，需要纠正大模型的错误 改变大模型回答的风格等等需要定制，就需要大模型微调 如果大家想要进阶动手微调一下的话，可以使用下面的框架，按照格式准备好数据即可，然后一键启动，上机器开始炼丹即可！ 目前主流的微调框架： https://github.com/hiyouga/LLaMA-Factory https://github.com/unslothai/unsloth https://github.com/modelscope/ms-swift RAGRAG &#x3D; LLM + 外置数据库 ​ 备注：这部分写于2025年4月16日 用过GPT的同学可能会了解到，GPT的知识具有时效性，比如GPT-4发布时间在2023年，那么GPT-4绝对无法帮你回答2024年之后的事情；再者，你有一个本地知识库的时候，你需要大模型结合这个知识库来回答你的问题时，你可能就需要RAG了； 右边严格来说不是RAG的流程，但是广义来讲，也算RAG，llm在其中扮演的是嘴替(总结)的作用；左边这个图是标准的RAG的流程，主要包括两个部分：index(入库) 和 query(出库) 如果大家想要深入学习这一部分的话，下面是一些可以进阶的部分： 当前两个主流的RAG搭建框架(RAG企业级开发)： langchain：https://github.com/langchain-ai/langchain llamaindex：https://github.com/run-llama/llama_index 可以学习的开源项目： QAnything: https://github.com/netease-youdao/QAnything RAGflow：https://github.com/infiniflow/ragflow/blob/main/README_zh.md 关于RAG的优化：RAG技术 AgentAgent &#x3D; LLM + 外部工具 上面的RAG是LLM和外部的知识(文档，图片等)打通了一条链路，相当于在数据层面建立了联系；但是不具备数据处理的能力，而处理数据的能力，一般是API是通过API的形式来展现的；**因此当LLM能够使用外部的 工具(API), LLM的能力将得到极大的扩展 ，**不多bb，展示！！！ https://strudel.cc/ (下面Agent使用到的一个音乐工具) 如果没有耐心看完前面Agent执行步骤的话，可直接拉到2：48秒，然后看后面的即可 如果给了LLM一台可以运行的电脑(环境)，那么这就是今年3月份爆火的AI智能体 Manus 为什么要叫Agent呢，可以看下面的图(没找到比较好的图，自己画的，见谅)，我们可以将用户比作boss，llm比作员工，boss发出一个问题之后，llm去规划并完成工作，llm是具体的执行者，因此叫做代理,Agent 暂时无法在飞书文档外展示此内容 目前智能Agent产品设计一般都是按照上面的思路去进行的，规划(Plan) + 执行(ReAct模型) Agent通过上面的章节，对Agent进行了一些初步的了解，这个小节将通过代码进一步的带大家了解Agent相关的内容 Agent 李宏毅讲Agent 从上面的一些例子我们可以了解到，对于完成一个人类目标而言，Agent需要两个方面的能力，目标拆解的能力和分布执行的能力；对应的名词即plan和function call，有了分步的任务，之后，LLM Agent逐步分析，调用工具或者自身内部知识，然后将结果传给下一步的任务，如下图所示： 通过上面的方式，可以有哪些运用呢？ 用模型训练模型： 使用Agent下围棋，使用电脑，浏览网页等等！ 当然上面的例子也说明了，大模型在调用过程中，可能出现幻觉，这有可能导致任务执行的失败，比如上面的deepseek 改变了国际象棋的规则，然后赢得了AI届的国际象棋桂冠！因此对于复杂问题，选择其它方法或许更可靠。这里不再展开。 上面都提到了大模型要实现Agent，需要具备工具调用的能力，这个能力是怎么实现的呢？看图好像挺简单的一句话就说完了，其实实际呢，也挺简单的。下面通过一个Function Call的例子来简单说明一下。 Function call 调用的实例代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import randomfrom datetime import datetimeimport json# step 1: 定义工具def get_current_time(): current_datetime = datetime.now() formatted_time = current_datetime.strftime(&#x27;%Y-%m-%d %H:%M:%S&#x27;) return f&quot;当前时间：&#123;formatted_time&#125;。&quot;def add(arguments): a = arguments[&quot;num_1&quot;] b = arguments[&quot;num_2&quot;] return f&quot;计算的结果是：&#123;a + b&#125;&quot;# step 2: 创建tools数组tools = [ &#123; &quot;type&quot;: &quot;function&quot;, &quot;function&quot;: &#123; &quot;name&quot;: &quot;get_current_time&quot;, &quot;description&quot;: &quot;当你想知道现在的时间时非常有用。&quot;, &#125; &#125;,]tool_name = [tool[&quot;function&quot;][&quot;name&quot;] for tool in tools]# step 3：使用大模型调用函数from openai import OpenAIimport osclient = OpenAI( api_key=&quot;sk-389c222d8f304e6ba3bb10ad3589d340&quot;, base_url=&quot;https://dashscope.aliyuncs.com/compatible-mode/v1&quot;,)messages = [ &#123; &quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;&quot;&quot;你是一个很有帮助的助手。如果用户提问关于时间的问题，请调用‘get_current_time’函数。 请以友好的语气回答问题。&quot;&quot;&quot;, &#125;, &#123; &quot;role&quot;: &quot;user&quot;, &quot;content&quot;: &quot;现在是几点？&quot; &#125;]def function_calling(): completion = client.chat.completions.create( model=&quot;qwen-max&quot;, messages=messages, tools=tools, ) res = completion.choices[0].message print(res.model_dump_json()) return completion# step4: 运行工具函数completion = function_calling()function_name = completion.choices[0].message.tool_calls[0].function.namearguments_string = completion.choices[0].message.tool_calls[0].function.arguments# 使用json模块解析参数字符串arguments = json.loads(arguments_string)# 创建一个函数映射表function_mapper = &#123; &quot;get_current_time&quot;: get_current_time, &quot;add&quot;:add&#125;# 获取函数实体function = function_mapper[function_name]# 如果入参为空，则直接调用函数if arguments == &#123;&#125;: function_output = function()# 否则，传入参数后调用函数else: function_output = function(arguments)# 打印工具的输出print(f&quot;工具函数输出：&#123;function_output&#125;\\n&quot;)## step5： 将工具输出添加到messages中，继续进行下面的步骤messages.append(completion.choices[0].message)messages.append(&#123;&quot;role&quot;: &quot;tool&quot;, &quot;content&quot;: function_output, &quot;tool_call_id&quot;: completion.choices[0].message.tool_calls[0].id&#125;)print(&quot;已添加tool message\\n&quot;)completion = function_calling() WorkFlow - 编排的Agent上面提到，对于复杂任务，完全让大模型去规划，去自由探索，可能出现很多不可预知的问题，这对于一些确定性的任务来说，是很致命的，既然这样，那么就别让大模型逞能，脚踏实地，只解决具体的单步问题，规划问题让人类来干就好了，这就是workflow，这就是截止到2025年4月20日来最主流的Agent 实现方式； MCP上面是通义千问的Function 调用方式，下次我们再添加一个新的工具的时候，是不是又要重复写一下这个流程(定义工具，定义工具描述，然后传给大模型)， 但是现在大模型这么多，大模型之间的Function Call的方式可能不相同，这里写的工具函数，换到GPT4，可能就要修改代码，这样很不符合程序员的复用规则，因此有了MCP MCP其实还是大模型选择需要调用的工具，因此MCP本质还是Function Call， 只是统一了一个标准之后，开发者通过MCP协议写的Function可以给别人复用了 如果想深入了解的话，大家可以对照着官方代码写一遍，就会发现了MCP解决的是哪一方面的问题了； A2A，ANP等等A2A：谷歌提出来的一种 Agent 和 Agent相互通信的协议， MCP协议统一了大模型与外界工具交互的方式，A2A是Agent与Agent间的，可以类比成公司里面部门，每个部门承担一部功能(如前端，后端，数据，算法，HR，财务，运维等)，然后相互协作，共同达成一个目标； ANP：国内提出的类似于A2A的，用于Agent2Agent的协议，目前是成为互联网界的http . . . . . 无论是MCP，还是A2A，抑或者是其它协议，我觉得归根到底是今年自deepseek以来，**大模型的能力得到了较大的提升，**这个生态才慢慢火起来(2024年开年火了一阵就没声音了，因为Agent有点人工智障的味道)，至于Agent有多智能，我觉得还是让子弹飞一会儿，但是在小的方面，确实会影响我们的工作效率；恰当的使用llm来协助我们工作，能做到事半功倍； 好物推荐编程工具类： Cursor (目前最好的AI代码编辑器，就是有点小贵，我使用的是VScode通义灵码插件替代) Trae (字节出品， 国产第一个有知名度的AI代码编辑器(当然，投流，广告投入很大)) 开发框架类:（还是上面那两个） langchain：https://github.com/langchain-ai/langchain llamaindex：https://github.com/run-llama/llama_index (个人常用这个，教程很友好， 但是目前langchain发展更好一些) 零代码类大模型编排工具(适合AI行业的所有人) 搞过comfyUI的同学应该知道节点编辑工具的概念，这个就是llm这边的节点编辑工具 dify : 企业级的，目前大多数工具开发AI流程应用的首选 n8n ：同上，优势在集成了很多的工具 扣子 ： 字节出品，和dify类似 举个dify的例子： 本来想演示一个使用dify搭建小影知识库问答系统的demo，但是貌似飞书文档权限比较严格&#x3D;_&#x3D;#，因此这里简单介绍一下； 以翻译的任务举个例子(目前主流翻译的流程)：通过搭积木的方式，20分钟(2分钟搭建，18分钟的提示词) 即可快速完成一个任务的搭建 另外，比较喜欢的一个功能是不同模型的对比，我觉得在验证阶段很实用 现场演示Agentcline + MCP MCP发展得如火如荼，开源社区上也有很多好玩的MCP工具了，大家可以尝试一下 想一想：大家现在看到这个热点，大家觉得是通过什么实现的？ difyima (好用的结合RAG的文档工具)","categories":[],"tags":[{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"LLM","slug":"LLM","permalink":"https://flippy-bird.github.io/tags/LLM/"}]},{"title":"AR的前世今生","slug":"AR的前世今生","date":"2022-09-27T07:58:23.000Z","updated":"2025-11-27T10:11:48.582Z","comments":true,"path":"2022/09/27/AR的前世今生/","permalink":"https://flippy-bird.github.io/2022/09/27/AR%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/","excerpt":"","text":"1.概念区分什么是AR&#x2F;VR&#x2F;MR&#x2F;XR? 在前段时间，元宇宙大火的时候，你应该听过或者见过上面的几个词语。什么？元宇宙都没有听过，建议食用下面的资料了解一下,也是蛮有意思的东西。 聊聊这个本不存在的“元宇宙” 啥？全是文字，没事，那就看个视频吧，你也可以简单了解一下这篇文章的主题AR是个什么东西！ 2021年了，AR眼镜可以做什么？ AR 对现实的一种增强，设备对现实识别（形状、位置、动作、边缘）,从而将虚拟信息叠加在现实中。 VR 为用户提供一个虚拟的世界，不必考虑现实世界，是一种沉浸式的体验。如果喜欢看动漫的，**动漫《刀剑神域》里描述就是这样一个世界(PS: 我感觉这就是当前阶段元宇宙的最终形态)；在电影《头号玩家》**中，里面塑造的绿洲也是这样的一个虚拟世界。 MR 虚拟世界和现实世界的混合。 AR、VR、MR统称XR。VR的发展相较于AR的发展，更为成熟一些，有很多VR面对用户的产品，例如Pico 4，这段时间发布了它的新产品。在这里，我们主要聊一聊AR相关的内容。 2.AR历史发展及现状1992年波音公司研究员提出了AR这个概念，自此AR诞生了。 2.1 AR序幕的开启把时间拉回10年前的2012年，在一个慈善晚宴上，谷歌联合创始人谢尔盖·布林为了展示一把谷歌的技术实力，就把尚在原型阶段的谷歌智能眼镜Google Glass掏了出来，众人直呼“卧槽”，随后便引起了大众的广泛讨论和关注。正当大家做着钢铁侠梦的时候，也遇到了视频中小何同学遇到的各种使用问题；虽然Google Glasss更多意义上只是一个“戴在脸上的手机”，很多AR领域涉及的技术，受限于当时的技术和算力，诸如环境检测，用户交互技术等并不涉及，但是其展现的前景还是值得大家期待的。因此，可以说，Google Glass拉起了AR快速发展的序幕。 2.2 AR的持续发展 AR界的第一代黑科技 自从谷歌将Google Glass引入大众的视野，旁边的微软可坐不住了，论技术，咱可没输过谁，就算打不过，那不是最后还可以加入嘛。况且，你的那个Google Glass也太垃圾了吧，这么重，还这么贵，也就是你是第一人，否则不会有那么多人买账，让我来教教你怎么制作一个好的产品吧，是时候展现真正的技术了。于是，在2015年的时候，微软发布了Hololens，震惊了世界，它能检测外界环境，将虚拟物体和显示环境融合，并且它通过追踪用户手势来完成交互。HoloLens2 是真正意义上的第一款相对成熟的AR设备，它的出现，引领了AR的潮流，成为了业界的标杆和对比的对象。 2019年，微软发布了HoloLens2，解决了视野狭小的问题，并且大幅度提升了手势追踪的准确性。虽然HoloLens2 虽然相对成熟，但是成本居高不下，**官方指导建议零售价是3500美元。**另外，虽然它相对体积较小，但仍重达500g, 已知iPhone 12重152g, 请问你的头上戴了几台iPhone 12 🤣 AR界的第一代黑”概念” 凭借着下面这“一条在体育馆地板跃起的大鲸鱼”，Magic Leap 成功塑造了一个 AR 黑科技公司的形象，以及足够激动人心的未来——从移动计算迁移到空间计算。裸眼AR的概念太具有震撼力，致使Magic Leap在视频发布的短短三个月内，迅速获得16亿美元融资，最高估值达64亿美元。然而巧妇难为无米之炊，在关键技术上没有取得核心突破的Magic Leap在2017年推出的Magic Leap one，其表现与其吹牛的效果差得太离谱，消费者纷纷给出了差评，这也给这家公司贴上了“骗子”的标签。 在上面的视频被人戳破，是视频后期合成之后，这家公司在AR界没有惊起任何浪花，就在写这篇文稿的当天，我在其官网上发现，Magic Leap将发布Magic Leap 2，这回他能否力挽狂澜，拯救自己“骗子”的形象，还得看过硬的产品！ 国内AR的发展 国内的AR公司主要有酷派的Xview系列、亮亮视野、影目科技等，要说目前最火的，当属Nreal， 在今年的8月23日，Nreal推出了新品Nreal Air和Nreal X两款AR眼镜；发布首日，Nreal销售额突破了1200万。 在9月21日，国内最早成立的AR企业之一亮亮视野发布了一款量产的AR字幕眼镜，也是全球第一款重量小于80g的双目波导无线一体机AR眼镜：听语者。主打听障者沟通。 国内的AR产业也在快速发展中，苹果老早就说2022年要发布AR眼镜，从年头等到了年尾，毛都没有；或许，我们可以把目光转向国内，期待国内的厂商能够发布一款改变世界的产品，就像当时iphone的出现一样。 2.3 移动端AR比较流行的SDK目前移动端AR SDK有很多，下面列出一些比较有影响力的SDK 苹果的ARKit ： 2017年发布至今，被认为是性能最强，最具商业潜力的sdk。 谷歌的ARCore ： 同时支持Android 和iOS, 是Android比较常用的sdk，综合实力比不上ARKit 历史悠久的 ARToolKit： 第一个AR SDK，但是目前开源社区不是太活跃 商业化sdk Vuforia： 性能表现很不错，支持Android、iOS 2.4 发展现状目前AR技术还有很多痛点，主要集中在以下几个方面： 检测环境的计算量很大，实时性不好，例如当你拿着设备快速移动时，虚拟物体和现实物体的融合就会出现飘移的现象 因为计算量大和频繁使用摄像头，耗电发热； 穿戴式AR 显示技术不成熟，大部分设备在清晰度、视野、体验性上都有些不足 软件生态不成熟 3.AR领域的关键技术3.1 探知环境的技术（自我定位、检测3D物体、环境建模） 没错，听到这些词语，很自然想到了自动驾驶里面也会有这样一个环节，就是”理解“环境的技术。 一般来说，建立环境地图需要经历以下的流程： 用户抵达一个新场所，开启摄像头。 设备根据陀螺仪等传感器确定自己的高度、朝向，并以此为空间坐标系原点。 识别用户摄像头输入，进行“点阵云”建模。 用户到处移动，直至整个地图被扫描建模完成。 3.2 显示技术AR的另一个关键技术就是显示技术，主要分为三类： 视觉差显示技术 全息投影技术 光影成像技术 3.3 交互技术目前为止，除了传统的触摸，按键、语音等技术，AR大致有3个方向的虚拟交互技术： 动作捕捉： 主要是手部动捕捉，目前Hololens2在这一块上技术积累最雄厚，不需要任何外设，直接通过手势就可以流畅操控设备。 眼动追踪： 使用摄像头捕捉人眼或脸部的图像，然后用算法实现人脸和人眼的检测、定位与跟踪，从而估算用户的视线变化。 脑机接口： 通过识别大脑活动电信号来操控设备，马斯克旗下的Neuralink公司通过向大脑植入十分之一发丝粗细的神经线，来检测和传输大脑的信号。 4.结语​ 20年前，当人们使用QICQ打着字，和天南海北的陌生人分享彼此的故事时，不会想到，今天我们一张图片，一个个表情包，一段视频，便可以轻松地将我们的故事分享给陌生人。20年前，我们省吃俭用抠抠搜搜计算着我们的2G流量，发个消息还老转圈圈，到今天，4G已悄然开启了全民直播时代，而5G也已在全面布局。 ​ 或许像电影《头号玩家》里面的元宇宙场景还需要很长一段路要走，但至少我觉得AR、VR技术将会在不久的将来带给我们全新的体验，就让我们且走，且看，且体验吧！","categories":[],"tags":[{"name":"元宇宙","slug":"元宇宙","permalink":"https://flippy-bird.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99/"},{"name":"AR","slug":"AR","permalink":"https://flippy-bird.github.io/tags/AR/"}]}],"categories":[{"name":"Agent开源项目学习","slug":"Agent开源项目学习","permalink":"https://flippy-bird.github.io/categories/Agent%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE%E5%AD%A6%E4%B9%A0/"}],"tags":[{"name":"Rag","slug":"Rag","permalink":"https://flippy-bird.github.io/tags/Rag/"},{"name":"Agent","slug":"Agent","permalink":"https://flippy-bird.github.io/tags/Agent/"},{"name":"MCP","slug":"MCP","permalink":"https://flippy-bird.github.io/tags/MCP/"},{"name":"LLM","slug":"LLM","permalink":"https://flippy-bird.github.io/tags/LLM/"},{"name":"元宇宙","slug":"元宇宙","permalink":"https://flippy-bird.github.io/tags/%E5%85%83%E5%AE%87%E5%AE%99/"},{"name":"AR","slug":"AR","permalink":"https://flippy-bird.github.io/tags/AR/"}]}